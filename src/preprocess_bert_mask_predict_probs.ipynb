{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "# from datasets.quora_bert_dataset import QuoraBertDataset\n",
    "from transformer.Models import Transformer, Encoder, Decoder\n",
    "# from transformer.Optim import ScheduledOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import os\n",
    "\n",
    "class QuoraBertMaskPredictProbDataset(Dataset):\n",
    "    def __init__(self, mode, train_size=5000, val_size=1000, test_size=1000, \n",
    "                 text_path='../data/quora_train.txt', pretrained_model_name='bert-base-cased', preprocessed_folder=None):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.tokenizer = self.init_tokenizer(pretrained_model_name)\n",
    "        self.mask_predict_model = BertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "        self.sentences = self.read_text(text_path)\n",
    "        self.init_constants()\n",
    "        \n",
    "        self.n_words = len(self.tokenizer)\n",
    "        \n",
    "        if preprocessed_folder is not None and os.path.exists(preprocessed_folder):\n",
    "            self.preprocessed_folder = preprocessed_folder\n",
    "        else:\n",
    "            self.preprocessed_folder = None\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.mask_predict_model = self.mask_predict_model.to(self.device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        seq1, seq2 = sentence.split('\\t')\n",
    "        \n",
    "        tokens1 = self.tokenizer.tokenize(seq1)\n",
    "        word_pieces1 =  [self.SOS_token] + tokens1 + [self.EOS_token]\n",
    "        idxes1 = self.tokenizer.convert_tokens_to_ids(word_pieces1)\n",
    "        \n",
    "        tokens2 = self.tokenizer.tokenize(seq2)\n",
    "        word_pieces2 = [self.SOS_token] + tokens2 + [self.EOS_token]\n",
    "        idxes2 = self.tokenizer.convert_tokens_to_ids(word_pieces2)\n",
    "        \n",
    "        seq1_tensor = torch.tensor(idxes1, dtype=torch.long)\n",
    "        seq2_tensor = torch.tensor(idxes2, dtype=torch.long)\n",
    "        \n",
    "        if self.preprocessed_folder is not None:\n",
    "            preprocessed_file = \"{}/{}.npy\".format(preprocessed_folder, idx)\n",
    "            mask_probs = np.load(preprocessed_file, allow_pickle=True)\n",
    "            mask_probs = torch.tensor(mask_probs)\n",
    "        else:\n",
    "            # mask each word from the begining to the end\n",
    "            # get the probability distribution of the mask tokens\n",
    "            pred = self.get_mask_pred_probs(seq1_tensor)\n",
    "            mask_probs = self.get_stack_probs(pred)\n",
    "        \n",
    "        return seq1_tensor, seq2_tensor, mask_probs\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_size\n",
    "        elif self.mode == 'val':\n",
    "            return self.val_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    # [CLS]  [M]  w2  w3  [SEP]        \n",
    "    # [CLS]  w1  [M]  w3  [SEP]        \n",
    "    # [CLS]  w1 w2  [M]  [SEP]        \n",
    "    # get the probability of [M] for each mask-prediction case\n",
    "    # mask for (number of words) times and every time get (number of words + 2) probability\n",
    "    # TODO: get the pred for only once\n",
    "    def get_mask_pred_probs(self, seq1):\n",
    "        mask_sentences = []\n",
    "        \n",
    "        for i in range(1, len(seq1) - 1):\n",
    "            mask_seq = seq1.detach().clone()\n",
    "            mask_seq[i] = self.MASK_token_id\n",
    "            mask_sentences.append(mask_seq)\n",
    "\n",
    "        mask_stack = torch.stack(mask_sentences)\n",
    "        mask_stack = mask_stack.to(self.device)\n",
    "        \n",
    "        self.mask_predict_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = self.mask_predict_model(mask_stack)[0]\n",
    "        pred = pred.cpu()\n",
    "        return pred\n",
    "    \n",
    "    def get_stack_probs(self, pred):\n",
    "        bos_prob = torch.zeros(self.n_words)\n",
    "        bos_prob[self.SOS_token_id] = 1\n",
    "\n",
    "        eos_prob = torch.zeros(self.n_words)\n",
    "        eos_prob[self.EOS_token_id] = 1\n",
    "\n",
    "        mask_preds = []\n",
    "        for idx in range(pred.shape[0]):\n",
    "            mask_preds.append(pred[idx][idx+1])\n",
    "        mask_stack = torch.stack(mask_preds)\n",
    "        mask_stack = torch.cat((bos_prob.reshape(1,-1), mask_stack, eos_prob.reshape(1,-1)))\n",
    "        return mask_stack\n",
    "    \n",
    "    def init_tokenizer(self, pretrained_model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    def init_constants(self):\n",
    "        PAD_id,  SOS_id, EOS_id, UNK_id = self.tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\"])\n",
    "        self.PAD_token_id = PAD_id\n",
    "        self.SOS_token_id = SOS_id\n",
    "        self.EOS_token_id = EOS_id\n",
    "        self.UNK_token_id = UNK_id\n",
    "        \n",
    "        self.PAD_token = '[PAD]'\n",
    "        self.SOS_token = '[CLS]'\n",
    "        self.EOS_token = '[SEP]'\n",
    "        self.UNK_token = '[UNK]'\n",
    "        \n",
    "        self.MASK_token = '[MASK]'\n",
    "        self.MASK_token_id = self.tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n",
    "\n",
    "        \n",
    "    def read_text(self, text_path):\n",
    "        # add words to dictionary\n",
    "        f = open(text_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        print(len(lines))\n",
    "        if self.mode == \"train\":\n",
    "            lines = lines[:self.train_size]\n",
    "        elif self.mode == 'val':\n",
    "            lines = lines[self.train_size:self.train_size+self.val_size]\n",
    "        else:\n",
    "            lines = lines[self.train_size+self.val_size:self.train_size+self.val_size+self.test_size]\n",
    "        \n",
    "        return lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149263\n"
     ]
    }
   ],
   "source": [
    "dataset = QuoraBertMaskPredictProbDataset(\"train\", 10000, 0, text_path='../data/quora_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_path = '../data/preprocess_quora_bert_mask_predict/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [08:47<00:00, 18.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# preprocess\n",
    "probs = []\n",
    "\n",
    "preprocessed_folder = '../data/preprocess_quora_bert_mask_predict/'\n",
    "\n",
    "# for i in tqdm(range(1000)):\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    _, _, prob = dataset[i]\n",
    "    prob_np = prob.cpu().numpy()\n",
    "    prob_tensor = torch.tensor(prob_np)\n",
    "    preprocessed_path = \"{}/{}.pt\".format(preprocessed_folder, i)\n",
    "    torch.save(prob_tensor, preprocessed_path)\n",
    "\n",
    "#     preprocessed_path = \"{}/{}.npy\".format(preprocessed_folder, i)\n",
    "#     np.save(preprocessed_path, prob_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_home/r08922168/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# probs_np = np.array(probs, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_path = '../data/preprocess_quora_all_bert_mask_predict.npy'\n",
    "# np.save(preprocessed_path, probs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149263\n"
     ]
    }
   ],
   "source": [
    "# p_dataset = QuoraBertMaskPredictProbDataset(\"train\", 149263, 0, text_path='../data/quora_train.txt', preprocessed_folder = preprocessed_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# # test speed\n",
    "# for i in tqdm(range(1000)):\n",
    "#      _, _, _ = dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 600.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm(range(1000)):\n",
    "#      _, _, _ = p_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
