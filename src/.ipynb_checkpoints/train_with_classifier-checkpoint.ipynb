{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Optim import ScheduledOptim\n",
    "from utils import cal_loss, cal_performance, log_performances_with_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse argument\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--lambda\", dest=\"lambda_\", type=float, default=10)\n",
    "\n",
    "args = parser.parse_args()\n",
    "lambda_ = args.lambda_\n",
    "print(\"lambda:\", lambda_)\n",
    "\n",
    "# lambda_ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed = 0\n",
    "same_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_model_path': '../models/DNPG_base_transformer_bert_tokenizer_with_classifier.pth', 'log_file': '../logs/DNPG_base_transformer_bert_tokenizer_with_classifier.txt', 'test_output_file': '../outputs/test_DNPG_transformer_bert_tokenizer_with_classifier_out.txt', 'val_output_file': '../outputs/val_DNPG_transformer_bert_tokenizer_with_classifier_out.txt', 'dataset': 'quora_bert_dataset', 'num_epochs': 50, 'batch_size': 50, 'd_model': 450, 'd_inner_hid': 512, 'd_k': 50, 'd_v': 50, 'n_head': 9, 'n_layers': 3, 'n_warmup_steps': 30000, 'dropout': 0.1, 'embs_share_weight': True, 'proj_share_weight': True, 'label_smoothing': False, 'train_size': 100000, 'val_size': 4000, 'test_size': 20000, 'is_bow': False, 'lr': '1e-3'}\n"
     ]
    }
   ],
   "source": [
    "##### Read Arguments from Config File #####\n",
    "\n",
    "# read from command line\n",
    "\n",
    "config_path = '../configs/dpng_transformer_bert_tokenizer_with_classifier.yaml'\n",
    "\n",
    "preprocessed = False\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config)\n",
    "\n",
    "save_model_path = config['save_model_path']\n",
    "log_file = config['log_file']\n",
    "use_dataset = config['dataset']\n",
    "\n",
    "num_epochs = config['num_epochs']\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "d_model = config['d_model']\n",
    "d_inner_hid = config['d_inner_hid']\n",
    "d_k = config['d_k']\n",
    "d_v = config['d_v']\n",
    "\n",
    "n_head = config['n_head']\n",
    "n_layers = config['n_layers']\n",
    "n_warmup_steps = config['n_warmup_steps']\n",
    "\n",
    "dropout = config['dropout']\n",
    "embs_share_weight = config['embs_share_weight']\n",
    "proj_share_weight = config['proj_share_weight']\n",
    "label_smoothing = config['label_smoothing']\n",
    "\n",
    "train_size = config['train_size']\n",
    "val_size = config['val_size']\n",
    "\n",
    "try:\n",
    "    is_bow = config['is_bow']\n",
    "\n",
    "    if is_bow:\n",
    "        bow_strategy = config['bow_strategy']\n",
    "        topk = config['topk']\n",
    "        if bow_strategy != 'simple_sum':\n",
    "            indiv_topk = config['indiv_topk']\n",
    "        else:\n",
    "            # not used but use default value for simplicity\n",
    "            indiv_topk = 50\n",
    "        \n",
    "        only_bow = config['only_bow']\n",
    "        replace_predict = config['replace_predict']\n",
    "        append_bow = config['append_bow']\n",
    "        \n",
    "except KeyError:\n",
    "    is_bow = False\n",
    "    \n",
    "try:\n",
    "    use_wordnet = config['use_wordnet']\n",
    "    indiv_k = config['indiv_k']\n",
    "    replace_origin = config['replace_origin']\n",
    "except KeyError:\n",
    "    use_wordnet = False\n",
    "\n",
    "# todo: add to params\n",
    "lr = float(config['lr'])\n",
    "# lr = 5e-4\n",
    "# ###################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# preprocessed = False\n",
    "if preprocessed:\n",
    "    from datasets.quora_preprocessed_dataset import QuoraPreprocessedDataset as Dataset\n",
    "else:\n",
    "    if use_dataset == 'quora_dataset':\n",
    "        from datasets.quora_dataset import QuoraDataset as Dataset\n",
    "    elif use_dataset == 'quora_bert_dataset':\n",
    "        from datasets.quora_bert_dataset import QuoraBertDataset as Dataset\n",
    "    elif use_dataset == 'quora_bert_mask_predict_dataset':\n",
    "        from datasets.quora_bert_mask_predict_dataset import QuoraBertMaskPredictDataset as Dataset\n",
    "    elif use_dataset == 'quora_word_mask_prediction_dataset':\n",
    "        from datasets.quora_word_mask_prediction_dataset import QuoraWordMaskPredictDataset as Dataset\n",
    "    elif use_dataset == 'quora_wordnet_dataset':\n",
    "        from datasets.quora_wordnet_dataset import QuoraWordnetDataset as Dataset\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset is not defined or not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    seq1_tensors = [s[0] for s in samples]\n",
    "    seq2_tensors = [s[1] for s in samples]\n",
    "\n",
    "    # zero pad\n",
    "    seq1_tensors = pad_sequence(seq1_tensors,\n",
    "                                  batch_first=True)\n",
    "\n",
    "    seq2_tensors = pad_sequence(seq2_tensors,\n",
    "                                  batch_first=True)    \n",
    "\n",
    "    return seq1_tensors, seq2_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessed:\n",
    "    model_name = config_path.split('/')[-1][:-5]\n",
    "    preprocessed_file = '../data/preprocess_all_{}.npy'.format(model_name)\n",
    "    dataset = Dataset(\"train\", train_size, val_size, preprocessed_file=preprocessed_file)\n",
    "    val_dataset = Dataset(\"val\", train_size, val_size, preprocessed_file=preprocessed_file)    \n",
    "elif is_bow:\n",
    "    dataset = Dataset(\n",
    "        \"train\", train_size, val_size, bow_strategy=bow_strategy, topk=topk, indiv_topk=indiv_topk, \n",
    "        only_bow=only_bow, use_origin=only_bow, replace_predict=replace_predict, append_bow=append_bow\n",
    "    )\n",
    "    # try not to replace predict when validation?\n",
    "    val_dataset = Dataset(\n",
    "        \"val\", train_size, val_size, bow_strategy=bow_strategy, topk=topk, indiv_topk=indiv_topk, \n",
    "        only_bow=only_bow, use_origin=only_bow, replace_predict=replace_predict, append_bow=append_bow\n",
    "    )\n",
    "elif use_wordnet:\n",
    "    dataset = Dataset(\"train\", train_size, val_size, indiv_k=indiv_k, replace_origin=replace_origin)\n",
    "    val_dataset = Dataset(\"val\", train_size, val_size)\n",
    "else:\n",
    "    dataset = Dataset(\"train\", train_size, val_size)\n",
    "    val_dataset = Dataset(\"val\", train_size, val_size)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    dataset.n_words,\n",
    "    dataset.n_words,\n",
    "    src_pad_idx=dataset.PAD_token_id,\n",
    "    trg_pad_idx=dataset.PAD_token_id,\n",
    "    trg_emb_prj_weight_sharing=proj_share_weight,\n",
    "    emb_src_trg_weight_sharing=embs_share_weight,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    d_model=d_model,\n",
    "    d_word_vec=d_model,\n",
    "    d_inner=d_inner_hid,\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = transformer.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09, lr=lr),\n",
    "    2.0, d_model, n_warmup_steps)\n",
    "# optimizer = optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"bert-base-cased\"\n",
    "num_labels = 2\n",
    "classifier = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name, num_labels=num_labels)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classify_batch(pred, seq2):\n",
    "    trg_seq = seq2.to(device)\n",
    "\n",
    "    pred_seq = pred.max(1)[1].to(device)\n",
    "    pred_seq = pred_seq.reshape(batch_size, -1)\n",
    "\n",
    "    concat_seqs = []\n",
    "\n",
    "    for tseq, pseq in zip(trg_seq, pred_seq):\n",
    "        nopad_trg = tseq[tseq.nonzero(as_tuple=True)]\n",
    "        concat_seq = torch.cat((nopad_trg, pseq))\n",
    "        concat_seqs.append(concat_seq)\n",
    "\n",
    "    concat_seqs = pad_sequence(concat_seqs,batch_first=True).long()\n",
    "    masks_tensors = torch.zeros(concat_seqs.shape,\n",
    "                                dtype=torch.long).to(device)\n",
    "    # let bert attends only not padding ones\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        concat_seqs != 0, 1)\n",
    "    \n",
    "    labels = torch.ones(len(trg_seq)).long().to(device)\n",
    "    \n",
    "    return concat_seqs, masks_tensors, labels\n",
    "\n",
    "def cal_classify_loss(pred, seq2):\n",
    "    concat_seqs, masks_tensors, labels = get_classify_batch(pred, seq2)\n",
    "    outputs = classifier(concat_seqs)\n",
    "    batch_loss = criterion(outputs[0], labels)\n",
    "    \n",
    "    return batch_loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train epoch\n",
    "def train_epoch(model, data_loader, optimizer, device, smoothing=False):\n",
    "    model.train()\n",
    "    total_seq_loss, n_word_total, n_word_correct, total_cls_loss, total_batch_loss = 0, 0, 0, 0, 0\n",
    "    trange = tqdm(data_loader)\n",
    "    \n",
    "    # debug\n",
    "    count = 0\n",
    "    \n",
    "    for seq1, seq2 in trange:\n",
    "        src_seq = seq1.to(device)\n",
    "        trg_seq = seq2[:, :-1].to(device)\n",
    "        gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        seq2seq_loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, dataset.PAD_token_id, smoothing) \n",
    "        \n",
    "        classify_loss = cal_classify_loss(pred, seq2)\n",
    "        \n",
    "        loss = seq2seq_loss + lambda_ * classify_loss\n",
    "        \n",
    "        loss.backward()\n",
    "#         optimizer.step()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_batch_loss += loss.item()\n",
    "        total_seq_loss += seq2seq_loss.item()\n",
    "        total_cls_loss += classify_loss.item()\n",
    "        \n",
    "        trange.set_postfix({\n",
    "            'classify_loss': classify_loss.item(),\n",
    "            'seq2seq_loss': seq2seq_loss.item()\n",
    "        })\n",
    "        \n",
    "        # debug\n",
    "        print('classify_loss', classify_loss.item(), 'seq2seq_loss', seq2seq_loss.item())\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "    loss_per_word = total_seq_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    avg_cls_loss = total_cls_loss/len(data_loader)\n",
    "    avg_batch_loss = total_batch_loss/len(data_loader)\n",
    "    \n",
    "    return loss_per_word, accuracy, avg_cls_loss, avg_batch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_data_loader, device):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_seq_loss, n_word_total, n_word_correct, total_cls_loss, total_batch_loss = 0, 0, 0, 0, 0\n",
    "    \n",
    "    trange = tqdm(val_data_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2 in trange:\n",
    "\n",
    "            src_seq = seq1.to(device)\n",
    "            trg_seq = seq2[:, :-1].to(device)\n",
    "            gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n",
    "            pred = model(src_seq, trg_seq)\n",
    "\n",
    "            seq2seq_loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, dataset.PAD_token_id, smoothing=False) \n",
    "        \n",
    "            classify_loss = cal_classify_loss(pred, seq2)\n",
    "\n",
    "            loss = seq2seq_loss + lambda_ * classify_loss\n",
    "\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_batch_loss += loss.item()\n",
    "            total_seq_loss += seq2seq_loss.item()\n",
    "            total_cls_loss += classify_loss.item()\n",
    "\n",
    "            trange.set_postfix({\n",
    "                'classify_loss': classify_loss.item(),\n",
    "                'seq2seq_loss': seq2seq_loss.item()\n",
    "            })\n",
    "\n",
    "    loss_per_word = total_seq_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    avg_cls_loss = total_cls_loss/len(val_data_loader)    \n",
    "    avg_batch_loss = total_batch_loss/len(val_data_loader)\n",
    "    \n",
    "    return loss_per_word, accuracy, avg_cls_loss, avg_batch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:23<00:00,  3.21it/s, classify_loss=0.756, seq2seq_loss=6.49e+3]\n",
      "  1%|▏         | 1/80 [00:00<00:11,  6.71it/s, classify_loss=0.739, seq2seq_loss=6.76e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Training)   ppl:  20301.76403, accuracy: 7.528 %, avg_cls_loss:  0.75905, avg_loss:  6607.91607, elapse: 623.908 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.56it/s, classify_loss=0.746, seq2seq_loss=6.11e+3]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Validation) ppl:  9952.02011, accuracy: 7.821 %, avg_cls_loss:  0.74849, avg_loss:  6098.32438, elapse: 9.350 sec\n",
      "\n",
      "Epoch 2 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 289/2000 [01:32<09:05,  3.14it/s, classify_loss=0.735, seq2seq_loss=5.72e+3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8ddde7e02331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     train_loss_per_word, train_accu, train_cls_loss, avg_train_loss = train_epoch(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7b5871513434>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, device, smoothing)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mclassify_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#         optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_update_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# debug\n",
    "log_file = '../logs/tmp.txt'\n",
    "f = open(log_file, 'w')\n",
    "best_loss = 99999\n",
    "\n",
    "f.write(\"Config: {}\\n\".format(config))\n",
    "\n",
    "# debug\n",
    "for epoch in range(1):\n",
    "# for epoch in range(num_epochs):\n",
    "    print(\"Epoch {} / {}\".format(epoch + 1, num_epochs))\n",
    "    start = time.time()\n",
    "    train_loss_per_word, train_accu, train_cls_loss, avg_train_loss = train_epoch(\n",
    "        model, data_loader, optimizer, device, smoothing=label_smoothing\n",
    "    )\n",
    "    \n",
    "    log_performances_with_cls(\n",
    "        'Training', train_loss_per_word, train_accu, train_cls_loss, avg_train_loss, start, f\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    valid_loss_per_word, valid_accu, valid_cls_loss, avg_valid_loss = eval_epoch(model, val_data_loader, device)\n",
    "\n",
    "    log_performances_with_cls(\n",
    "        'Validation', valid_loss_per_word, valid_accu, valid_cls_loss, avg_valid_loss, start, f\n",
    "    )    \n",
    "    \n",
    "    if avg_valid_loss < best_loss:\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        best_loss = avg_valid_loss\n",
    "        print(\"model saved in Epoch {}\".format(epoch + 1))\n",
    "        f.write(\"model saved in Epoch {}\\n\".format(epoch + 1))\n",
    "        f.flush()\n",
    "\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
