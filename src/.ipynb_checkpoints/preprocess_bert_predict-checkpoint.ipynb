{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from datasets.quora_bert_mask_predict_dataset import QuoraBertMaskPredictDataset as Dataset\n",
    "from utils import same_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ArgumentParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3802022a0f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# parse argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m parser.add_argument(\"--config_file\", dest=\"config_file\",\n\u001b[1;32m      4\u001b[0m                     default='../configs/dpng_transformer_bert_tokenizer_bow_indivtopk.yaml')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ArgumentParser' is not defined"
     ]
    }
   ],
   "source": [
    "# parse argument\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--config_path\", dest=\"config_path\",\n",
    "                    default='../configs/dpng_transformer_bert_tokenizer_bow_indivtopk.yaml')\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", default=0, type=int)\n",
    "\n",
    "args = parser.parse_args()\n",
    "config_path = args.config_path\n",
    "seed = args.seed\n",
    "print(\"config_path:\", config_path)\n",
    "print(\"seed: \", seed)\n",
    "\n",
    "same_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_model_path': '../models/DNPG_base_transformer_bert_tokenizer_bert_bow_indivtopk.pth', 'log_file': '../logs/DNPG_base_transformer_bert_tokenizer_training_bert_bow_indiv_topk.txt', 'test_output_file': '../outputs/test_DNPG_transformer_bert_tokenizer_bow_indivtopk_out.txt', 'val_output_file': '../outputs/val_DNPG_transformer_bert_tokenizer_bow_indivtopk_out.txt', 'dataset': 'quora_bert_mask_predict_dataset', 'num_epochs': 50, 'batch_size': 100, 'd_model': 450, 'd_inner_hid': 512, 'd_k': 50, 'd_v': 50, 'n_head': 9, 'n_layers': 3, 'n_warmup_steps': 12000, 'dropout': 0.1, 'embs_share_weight': True, 'proj_share_weight': True, 'label_smoothing': False, 'train_size': 100000, 'val_size': 4000, 'test_size': 20000, 'is_bow': True, 'bow_strategy': 'indiv_topk', 'indiv_topk': 10, 'topk': 50, 'only_bow': True}\n"
     ]
    }
   ],
   "source": [
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk_onlybow.yaml'\n",
    "\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config)\n",
    "    \n",
    "train_size = config['train_size']\n",
    "val_size = config['val_size']\n",
    "test_size = config['test_size']\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "is_bow = config['is_bow']\n",
    "bow_strategy = config['bow_strategy']\n",
    "topk = config['topk']\n",
    "if bow_strategy != 'simple_sum':\n",
    "    indiv_topk = config['indiv_topk']\n",
    "else:\n",
    "    # not used but use default value for simplicity\n",
    "    indiv_topk = 50\n",
    "\n",
    "\n",
    "try:\n",
    "    is_bow = config['is_bow']\n",
    "\n",
    "    if is_bow:\n",
    "        bow_strategy = config['bow_strategy']\n",
    "        topk = config['topk']\n",
    "        if bow_strategy != 'simple_sum':\n",
    "            indiv_topk = config['indiv_topk']\n",
    "        else:\n",
    "            # not used but use default value for simplicity\n",
    "            indiv_topk = 50\n",
    "        \n",
    "        only_bow = config['only_bow']\n",
    "        replace_predict = config['replace_predict']\n",
    "\n",
    "\n",
    "except KeyError:\n",
    "    is_bow = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config_path.split('/')[-1][:-5]\n",
    "output_all_path = '../data/preprocess_all_{}.npy'.format(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# just init one dataset / bert model\n",
    "dataset = Dataset(\n",
    "    \"train\", train_size+val_size+test_size, 0, 0, bow_strategy=bow_strategy, topk=topk, indiv_topk=indiv_topk,\n",
    "    only_bow=only_bow, use_origin=only_bow, replace_predict=replace_predict\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tensors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124000/124000 [47:22<00:00, 43.62it/s] \n"
     ]
    }
   ],
   "source": [
    "for seq1_tensor, seq2_tensor in tqdm(data_loader):\n",
    "    seq1_tensor = seq1_tensor.view(-1)\n",
    "    seq2_tensor = seq2_tensor.view(-1)\n",
    "    sentence_tensors.append([seq1_tensor, seq2_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(sentence_tensors, dtype=object)\n",
    "np.save(output_all_path, arr)\n",
    "\n",
    "# load_arr = np.load(output_all_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[train_size+val_size+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
