{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: use id to select the sentence to analyze\n",
    "# to do: use translator to predict sentence\n",
    "import os\n",
    "import torch\n",
    "import time \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Translator import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_model_path': '../models/DNPG_base_transformer_bert_tokenizer_bert_bow.pth', 'log_file': '../logs/DNPG_base_transformer_bert_tokenizer_training_bert_bow.txt', 'test_output_file': '../outputs/test_DNPG_transformer_bert_tokenizer_bow_out.txt', 'val_output_file': '../outputs/val_DNPG_transformer_bert_tokenizer_bow_out.txt', 'dataset': 'quora_bert_mask_predict_dataset', 'num_epochs': 50, 'batch_size': 100, 'd_model': 450, 'd_inner_hid': 512, 'd_k': 50, 'd_v': 50, 'n_head': 9, 'n_layers': 3, 'n_warmup_steps': 12000, 'dropout': 0.1, 'embs_share_weight': True, 'proj_share_weight': True, 'label_smoothing': False, 'train_size': 100000, 'val_size': 4000, 'test_size': 20000, 'is_bow': True, 'bow_strategy': 'simple_sum', 'indiv_topk': 10, 'topk': 50, 'lr': '1e-3', 'only_bow': False, 'replace_predict': False}\n"
     ]
    }
   ],
   "source": [
    "##### Read Arguments from Config File #####\n",
    "\n",
    "# config_path = '../configs/base_transformer.yaml'\n",
    "# config_path = '../configs/dpng_transformer.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow.yaml'\n",
    "config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indiv_neighbors.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_maskword_indivtopk.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk_replace_nopreprocess.yaml'\n",
    "\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config)\n",
    "\n",
    "save_model_path = config['save_model_path']\n",
    "output_file = config['test_output_file']\n",
    "use_dataset = config['dataset']\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "d_model = config['d_model']\n",
    "d_inner_hid = config['d_inner_hid']\n",
    "d_k = config['d_k']\n",
    "d_v = config['d_v']\n",
    "\n",
    "n_head = config['n_head']\n",
    "n_layers = config['n_layers']\n",
    "n_warmup_steps = config['n_warmup_steps']\n",
    "\n",
    "dropout = config['dropout']\n",
    "embs_share_weight = config['embs_share_weight']\n",
    "proj_share_weight = config['proj_share_weight']\n",
    "label_smoothing = config['label_smoothing']\n",
    "\n",
    "train_size = config['train_size']\n",
    "val_size = config['val_size']\n",
    "test_size = config['test_size']\n",
    "\n",
    "beam_size = 1\n",
    "max_seq_len = 30\n",
    "\n",
    "try:\n",
    "    is_bow = config['is_bow']\n",
    "\n",
    "    if is_bow:\n",
    "        bow_strategy = config['bow_strategy']\n",
    "        topk = config['topk']\n",
    "        if bow_strategy != 'simple_sum':\n",
    "            indiv_topk = config['indiv_topk']\n",
    "        else:\n",
    "            # not used but use default value for simplicity\n",
    "            indiv_topk = 50\n",
    "\n",
    "except KeyError:\n",
    "    is_bow = False\n",
    "    \n",
    "try:\n",
    "    is_bow = config['is_bow']\n",
    "\n",
    "    if is_bow:\n",
    "        bow_strategy = config['bow_strategy']\n",
    "        topk = config['topk']\n",
    "        if bow_strategy != 'simple_sum':\n",
    "            indiv_topk = config['indiv_topk']\n",
    "        else:\n",
    "            # not used but use default value for simplicity\n",
    "            indiv_topk = 50\n",
    "        \n",
    "        only_bow = config['only_bow']\n",
    "        replace_predict = config['replace_predict']\n",
    "\n",
    "except KeyError:\n",
    "    is_bow = False\n",
    "    \n",
    "# ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "if preprocessed:\n",
    "    from datasets.quora_preprocessed_dataset import QuoraPreprocessedDataset as Dataset\n",
    "else:\n",
    "    if use_dataset == 'quora_dataset':\n",
    "        from datasets.quora_dataset import QuoraDataset as Dataset\n",
    "    elif use_dataset == 'quora_bert_dataset':\n",
    "        from datasets.quora_bert_dataset import QuoraBertDataset as Dataset\n",
    "    elif use_dataset == 'quora_bert_mask_predict_dataset':\n",
    "        from datasets.quora_bert_mask_predict_dataset import QuoraBertMaskPredictDataset as Dataset\n",
    "    elif use_dataset == 'quora_word_mask_prediction_dataset':\n",
    "        from datasets.quora_word_mask_prediction_dataset import QuoraWordMaskPredictDataset as Dataset\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset is not defined or not implemented\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if preprocessed:\n",
    "    model_name = config_path.split('/')[-1][:-5]\n",
    "    preprocessed_file = '../data/preprocess_all_{}.npy'.format(model_name)\n",
    "    dataset = Dataset(\"test\", train_size, val_size, test_size, preprocessed_file=preprocessed_file)\n",
    "elif is_bow:\n",
    "    dataset = Dataset(\n",
    "        \"test\", train_size, val_size, test_size, bow_strategy=bow_strategy, topk=topk, \n",
    "        indiv_topk=indiv_topk, only_bow=only_bow, use_origin=only_bow, replace_predict=replace_predict\n",
    "    )\n",
    "else:\n",
    "    dataset = Dataset(\"test\", train_size, val_size, test_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (src_word_emb): Embedding(28996, 450, padding_idx=0)\n",
       "    (position_enc): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_stack): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=450, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=450, bias=True)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=450, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=450, bias=True)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=450, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=450, bias=True)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (trg_word_emb): Embedding(28996, 450, padding_idx=0)\n",
       "    (position_enc): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_stack): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (enc_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=450, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=450, bias=True)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (enc_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=450, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=450, bias=True)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (enc_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_ks): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (w_vs): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (fc): Linear(in_features=450, out_features=450, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=450, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=450, bias=True)\n",
       "          (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((450,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (trg_word_prj): Linear(in_features=450, out_features=28996, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Transformer(\n",
    "    dataset.n_words,\n",
    "    dataset.n_words,\n",
    "    src_pad_idx=dataset.PAD_token_id,\n",
    "    trg_pad_idx=dataset.PAD_token_id,\n",
    "    trg_emb_prj_weight_sharing=proj_share_weight,\n",
    "    emb_src_trg_weight_sharing=embs_share_weight,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    d_model=d_model,\n",
    "    d_word_vec=d_model,\n",
    "    d_inner=d_inner_hid,\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout    \n",
    ")\n",
    "\n",
    "model = transformer.to(device)\n",
    "\n",
    "model.load_state_dict((torch.load(\n",
    "        save_model_path, map_location=device)))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This module will handle the text generation with beam search. '''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformer.Models import Transformer, get_pad_mask, get_subsequent_mask\n",
    "\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    ''' Load a trained model and translate in beam search fashion. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, model, beam_size, max_seq_len,\n",
    "            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n",
    "        \n",
    "\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.alpha = 0.7\n",
    "        self.beam_size = beam_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_bos_idx = trg_bos_idx\n",
    "        self.trg_eos_idx = trg_eos_idx\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n",
    "        self.register_buffer(\n",
    "            'blank_seqs', \n",
    "            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n",
    "        self.blank_seqs[:, 0] = self.trg_bos_idx\n",
    "        self.register_buffer(\n",
    "            'len_map', \n",
    "            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n",
    "\n",
    "\n",
    "    def _model_decode(self, trg_seq, enc_output, src_mask):\n",
    "        trg_mask = get_subsequent_mask(trg_seq)\n",
    "#         dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask, return_attns=True)\n",
    "        dec_output, dec_slf_attn_list, dec_enc_attn_list = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask, return_attns=True)\n",
    "        \n",
    "        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1), dec_enc_attn_list\n",
    "\n",
    "\n",
    "    def _get_init_state(self, src_seq, src_mask):\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n",
    "        dec_output, dec_enc_attn_list = self._model_decode(self.init_seq, enc_output, src_mask)\n",
    "        \n",
    "        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        scores = torch.log(best_k_probs).view(beam_size)\n",
    "        gen_seq = self.blank_seqs.clone().detach()\n",
    "        gen_seq[:, 1] = best_k_idx[0]\n",
    "        enc_output = enc_output.repeat(beam_size, 1, 1)\n",
    "        return enc_output, gen_seq, scores, dec_enc_attn_list\n",
    "\n",
    "\n",
    "    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n",
    "        assert len(scores.size()) == 1\n",
    "        \n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        # Get k candidates for each beam, k^2 candidates in total.\n",
    "        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        # Include the previous scores.\n",
    "        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n",
    "\n",
    "        # Get the best k candidates from k^2 candidates.\n",
    "        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n",
    " \n",
    "        # Get the corresponding positions of the best k candidiates.\n",
    "        best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n",
    "        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n",
    "\n",
    "        # Copy the corresponding previous tokens.\n",
    "        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n",
    "        # Set the best tokens in this beam search step\n",
    "        gen_seq[:, step] = best_k_idx\n",
    "\n",
    "        return gen_seq, scores\n",
    "\n",
    "\n",
    "    def translate_sentence(self, src_seq, return_attn=False):\n",
    "        # Only accept batch size equals to 1 in this function.\n",
    "        # TODO: expand to batch operation.\n",
    "        assert src_seq.size(0) == 1\n",
    "\n",
    "        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n",
    "        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n",
    "        \n",
    "        with torch.no_grad():\n",
    "        #     pred, enc_slf_attn_list, dec_slf_attn_list, dec_enc_attn_list = model(src_seq, trg_seq, True)\n",
    "        #     pred_seq = translator.translate_sentence(src_seq)\n",
    "            src_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "            enc_output, gen_seq, scores, dec_enc_attn_list = translator._get_init_state(src_seq, src_mask)\n",
    "\n",
    "            output_dec_enc_attn = [[] for _ in range(len(dec_enc_attn_list))]\n",
    "            \n",
    "            for l in range(len(dec_enc_attn_list)):\n",
    "                    output_dec_enc_attn[l].append(dec_enc_attn_list[l][0,:,-1,:])    \n",
    "            ans_idx = 0   # default\n",
    "            for step in range(2, max_seq_len):    # decode up to max length\n",
    "                dec_output, dec_enc_attn_list = translator._model_decode(gen_seq[:, :step], enc_output, src_mask)\n",
    "                gen_seq, scores = translator._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n",
    "\n",
    "                # append dec_enc_attn_list\n",
    "                for l in range(len(dec_enc_attn_list)):\n",
    "                    output_dec_enc_attn[l].append(dec_enc_attn_list[l][0,:,-1,:])\n",
    "\n",
    "                # Check if all path finished\n",
    "                # -- locate the eos in the generated sequences\n",
    "                eos_locs = gen_seq == trg_eos_idx   \n",
    "                # -- replace the eos with its position for the length penalty use\n",
    "                seq_lens, _ = translator.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n",
    "                # -- check if all beams contain eos\n",
    "                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n",
    "                    # TODO: Try different terminate conditions.\n",
    "                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n",
    "                    ans_idx = ans_idx.item()\n",
    "                    break\n",
    "\n",
    "\n",
    "        if return_attn:\n",
    "            for l in range(len(output_dec_enc_attn)):\n",
    "                output_dec_enc_attn[l] = torch.stack(output_dec_enc_attn[l], dim=1)\n",
    "            return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist(), output_dec_enc_attn\n",
    "        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = dataset.PAD_token_id\n",
    "trg_pad_idx = dataset.PAD_token_id\n",
    "    \n",
    "trg_bos_idx = dataset.SOS_token_id\n",
    "trg_eos_idx = dataset.EOS_token_id\n",
    "unk_idx = dataset.UNK_token_id\n",
    "\n",
    "# translator = Translator(\n",
    "#         model=model,\n",
    "#         beam_size=3,\n",
    "#         max_seq_len=max_seq_len,\n",
    "#         src_pad_idx=src_pad_idx,\n",
    "#         trg_pad_idx=trg_pad_idx,\n",
    "#         trg_bos_idx=trg_bos_idx,\n",
    "#         trg_eos_idx=trg_eos_idx).to(device)\n",
    "\n",
    "translator = Translator(\n",
    "        model=model,\n",
    "        beam_size=1,\n",
    "        max_seq_len=max_seq_len,\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        trg_bos_idx=trg_bos_idx,\n",
    "        trg_eos_idx=trg_eos_idx).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trans_dec_enc_attn = [[] for _ in range(3)]\n",
    "# for l in range(len(output_dec_enc_attn)):\n",
    "#     output_dec_enc_attn[l] = torch.stack(output_dec_enc_attn[l], dim=1)\n",
    "# # output_dec_enc_attn = torch.stack(output_dec_enc_attn[0], dim=1)\n",
    "# print(dec_enc_attn_list[0].shape)\n",
    "# print(dec_enc_attn_list[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     def translate_sentence(self, src_seq):\n",
    "#         # Only accept batch size equals to 1 in this function.\n",
    "#         # TODO: expand to batch operation.\n",
    "#         assert src_seq.size(0) == 1\n",
    "\n",
    "#         src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n",
    "#         max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             src_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "#             enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n",
    "\n",
    "#             ans_idx = 0   # default\n",
    "#             for step in range(2, max_seq_len):    # decode up to max length\n",
    "#                 dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n",
    "#                 gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n",
    "\n",
    "#                 # Check if all path finished\n",
    "#                 # -- locate the eos in the generated sequences\n",
    "#                 eos_locs = gen_seq == trg_eos_idx   \n",
    "#                 # -- replace the eos with its position for the length penalty use\n",
    "#                 seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n",
    "#                 # -- check if all beams contain eos\n",
    "#                 if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n",
    "#                     # TODO: Try different terminate conditions.\n",
    "#                     _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n",
    "#                     ans_idx = ans_idx.item()\n",
    "#                     break\n",
    "#         return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "seq1, seq2 = dataset[idx]\n",
    "\n",
    "seq1 = seq1.reshape(1, -1)\n",
    "seq2 = seq2.reshape(1, -1)\n",
    "\n",
    "src_seq = seq1.to(device)\n",
    "trg_seq = seq2[:,:-1].to(device)\n",
    "gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # shape of output_dec_enc_attn:\n",
    "    # list of layers with length = number of layers (3 here)\n",
    "    # layer * ( beam_size * number of heads * output length *  input length)\n",
    "    pred_seq, output_dec_enc_attn = translator.translate_sentence(src_seq, return_attn=True)\n",
    "    \n",
    "    # pred, enc_slf_attn_list, dec_slf_attn_list, dec_enc_attn_list = model(src_seq, trg_seq, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QuoraBertMaskPredictDataset' object has no attribute 'idx2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-629f2b6568dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# not bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# output = [dataset.idx2word[idx] for idx in trg_seq[0].tolist()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-629f2b6568dd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# not bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# output = [dataset.idx2word[idx] for idx in trg_seq[0].tolist()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QuoraBertMaskPredictDataset' object has no attribute 'idx2word'"
     ]
    }
   ],
   "source": [
    "# # not bert\n",
    "# src = [dataset.idx2word[idx] for idx in src_seq[0].tolist()]\n",
    "# output = [dataset.idx2word[idx] for idx in pred_seq]\n",
    "# # output = [dataset.idx2word[idx] for idx in trg_seq[0].tolist()]\n",
    "# print(\"src: \", src)\n",
    "# print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src:  ['[CLS]', 'What', 'is', 'the', 'food', 'you', 'can', 'eat', 'every', 'day', 'for', 'breakfast', ',', 'lunch', 'and', 'dinner', '?', '[SEP]', ',', 'and', 'that', '-', '...', 'or', 'all', 'breakfast', 'dinner', '?', 'for', 'in', 'with', 'like', 'this', 'one', 'lunch', 'on', '.', 'eat', 'then', 'after', 'it', 'to', 'of', 'so', 'about', 'the', 'you', 'now', 'here', 'what', 'at', 'food', 'more', 'is', 'before', 'tomorrow', 'as', 'meal', 'day', 'home', ':', 'night', 'there', 'not', 'most', 'eating', 'just', 'last', '[SEP]']\n",
      "Output:  ['[CLS]', 'What', 'is', 'the', 'best', 'food', 'you', 'can', 'eat', 'every', 'day', 'for', 'breakfast', '?', '[SEP]']\n",
      "Target:  ['[CLS]', 'Is', 'it', 'healthy', 'to', 'eat', 'fish', 'every', 'day', '?']\n"
     ]
    }
   ],
   "source": [
    "# bert\n",
    "tokenizer = dataset.tokenizer\n",
    "src = tokenizer.convert_ids_to_tokens(src_seq[0])\n",
    "output = tokenizer.convert_ids_to_tokens(pred_seq)\n",
    "target = tokenizer.convert_ids_to_tokens(trg_seq[0])\n",
    "\n",
    "print(\"Src: \", src)\n",
    "print(\"Output: \", output)\n",
    "print(\"Target: \", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting layer 0\n"
     ]
    }
   ],
   "source": [
    "# layers\n",
    "savefig = True\n",
    "model_name = config_path.split('/')[-1][:-5]\n",
    "# l: number of layers\n",
    "for l in range(3):\n",
    "    print(\"Plotting layer {}\".format(l))\n",
    "    fig, ((ax1), (ax2), (ax3), (ax4), (ax5), (ax6), (ax7), (ax8), (ax9)) = plt.subplots(9, 1, figsize=(15, 45))\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n",
    "\n",
    "    # i: number of heads\n",
    "    for i in range(len(axes)):\n",
    "#         beam_id = 0\n",
    "#         attention = dec_enc_attn_list[l][beam_id].cpu()[i]\n",
    "        attention = output_dec_enc_attn[l][i].cpu()\n",
    "\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(attention, cmap=\"YlGn\")\n",
    "\n",
    "        # Create colorbar\n",
    "        cbar = ax.figure.colorbar(im, ax=ax)\n",
    "        cbar.ax.set_ylabel(\"weight\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "        ax.set_xticks(np.arange(len(src)))\n",
    "        ax.set_yticks(np.arange(len(output)))\n",
    "\n",
    "        ax.set_xticklabels(src)\n",
    "        ax.set_yticklabels(output)\n",
    "\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "        for edge, spine in ax.spines.items():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        ax.set_xticks(np.arange(attention.shape[1]+1)-.5, minor=True)\n",
    "        ax.set_yticks(np.arange(attention.shape[0]+1)-.5, minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "        ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "        ax.set_title(f'Head {i}')\n",
    "        fig.tight_layout()\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        output_folder = \"../images/{}/idx_{}\".format(model_name, idx)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        output_name = \"{}/layer_{}\".format(output_folder, l)\n",
    "#         output_name = \"../images/attn_{}_layer_{}.png\".format(idx, l)\n",
    "        plt.savefig(output_name)\n",
    "        print(\"Image saved... {}\".format(output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
