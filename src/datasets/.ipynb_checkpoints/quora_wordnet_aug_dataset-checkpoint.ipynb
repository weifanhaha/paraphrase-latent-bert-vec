{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.corpus import wordnet \n",
    "import random\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dictionary\n",
    "# from quora_dataset import QuoraDataset\n",
    "from nltk.corpus import stopwords\n",
    "# called from src\n",
    "from datasets.quora_dataset import QuoraDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraWordnetAugDataset(Dataset):\n",
    "    def __init__(self, mode, train_size=5000, val_size=1000, test_size=1000, text_path='../data/quora_train.txt',\n",
    "                 word2idx_path='../data/word2idx.npy', idx2word_path='../data/idx2word_path.npy',\n",
    "                 dic_sentences_num=150000, load_dic=True, replace_prob=0.5):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.replace_prob = replace_prob\n",
    "        self.sentences = []\n",
    "\n",
    "        self._init_constants()\n",
    "        self._init_sentences(text_path)\n",
    "        self.word2idx, self.idx2word = QuoraDataset.build_dictionary(\n",
    "            text_path=text_path, sentences_num=dic_sentences_num, load_dic=load_dic,\n",
    "            word2idx_path=word2idx_path, idx2word_path=idx2word_path\n",
    "        )\n",
    "        self.n_words = len(self.word2idx)\n",
    "        self.stopwords = stopwords.words('english')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq1, seq2 = self.sentences[idx]\n",
    "        \n",
    "        idxes1 = [self._get_index(word) for word in seq1.split(' ')]\n",
    "        idxes1 = [self.SOS_token_id] + idxes1 + [self.EOS_token_id]\n",
    "        \n",
    "#         idxes2 = [self._get_index(word) for word in seq2.split(' ')]\n",
    "#         idxes2 = [self.SOS_token_id] + idxes2 + [self.EOS_token_id]\n",
    "        idxes2 = self._get_aug_sentence(seq2)\n",
    "        idxes2 = [self.SOS_token_id] + idxes2 + [self.EOS_token_id]\n",
    "\n",
    "        seq1_tensor = torch.tensor(idxes1, dtype=torch.long)        \n",
    "        seq2_tensor = torch.tensor(idxes2, dtype=torch.long)\n",
    "        \n",
    "        return (seq1_tensor, seq2_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_size\n",
    "        elif self.mode == 'val':\n",
    "            return self.val_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "    \n",
    "    def _get_index(self, word):\n",
    "        try: \n",
    "            index = self.word2idx[word]\n",
    "        except KeyError:\n",
    "            index = self.UNK_token_id\n",
    "        return index\n",
    "\n",
    "    def _init_constants(self):\n",
    "        self.PAD_token = '<PAD>'\n",
    "        self.SOS_token = '<SOS>'\n",
    "        self.EOS_token = '<EOS>'\n",
    "        self.UNK_token = '<UNK>'\n",
    "        self.PAD_token_id = 0\n",
    "        self.SOS_token_id = 1\n",
    "        self.EOS_token_id = 2\n",
    "        self.UNK_token_id = 3\n",
    "    \n",
    "    def _init_sentences(self, text_path):\n",
    "        f = open(text_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        # shuffle\n",
    "        np.random.shuffle(lines)\n",
    "        if self.mode == \"train\":\n",
    "            lines = lines[:self.train_size]\n",
    "        elif self.mode == 'val':\n",
    "            lines = lines[self.train_size:self.train_size+self.val_size]\n",
    "        else:\n",
    "            lines = lines[self.train_size+self.val_size:self.train_size+self.val_size+self.test_size]\n",
    "        \n",
    "        def normalize_sentence(s):\n",
    "            s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "            s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "            s = s.lower()\n",
    "            return s\n",
    "\n",
    "        for line in tqdm(lines):\n",
    "            seq1, seq2 = [normalize_sentence(seq) for seq in line.split('\\t')]\n",
    "            self.sentences.append((seq1, seq2))\n",
    "\n",
    "    def _get_aug_sentence(self, seq):\n",
    "        words = seq.split()\n",
    "        new_sentence_idxes = []\n",
    "        \n",
    "        for word in words:\n",
    "            syn_set = self._get_synset(word)\n",
    "            \n",
    "            if word in self.stopwords or len(syn_set) == 0:\n",
    "                new_sentence_idxes.append(self._get_index(word))\n",
    "                continue\n",
    "\n",
    "            # replace with prob \n",
    "            if random.random() > self.replace_prob:\n",
    "                new_sentence_idxes.append(self._get_index(word))\n",
    "                continue\n",
    "            \n",
    "            sampled_syn = random.sample(syn_set, 1)[0]\n",
    "            if self._get_index(sampled_syn) != self.UNK_token_id:\n",
    "                new_sentence_idxes.append(self._get_index(sampled_syn))\n",
    "            else:\n",
    "                new_sentence_idxes.append(self._get_index(word))\n",
    "                \n",
    "        return new_sentence_idxes\n",
    "    \n",
    "    def _get_synset(self, word):\n",
    "        syn_set = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas(): \n",
    "                # ignore the phrase\n",
    "                if '_' in l.name() or '-' in l.name():\n",
    "                    continue\n",
    "                syn_set.add(l.name()) \n",
    "        if word in syn_set:\n",
    "            syn_set.remove(word)\n",
    "        return syn_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 35932.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loading the Dictionary...\n",
      "[Info] Dictionary Loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset = QuoraWordnetAugDataset(\"train\", 50000, 1000, text_path='../../data/quora_train.txt', load_dic=True,\n",
    "# word2idx_path='../../data/word2idx.npy', idx2word_path='../../data/idx2word_path.npy')\n",
    "# # # dataset = QuoraDataset(\"train\", 50000, 1000, load_dic=False)\n",
    "# # word2idx, idx2word = QuoraDataset.build_dictionary(sentences_num=100000, load_dic=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################\n",
      "<SOS> which has more career opportunities biotechnology or biomedical science ? <EOS>\n",
      "<SOS> which is better bioengineering or biomedical science ? <EOS>\n",
      "-------------------\n",
      "<SOS> why do people drink coffee ? <EOS>\n",
      "<SOS> why do so many people enjoy drinking coffee ? <EOS>\n",
      "-------------------\n",
      "<SOS> what are the best novels in english ? <EOS>\n",
      "<SOS> what are the best novels in english ? and why ? <EOS>\n",
      "-------------------\n",
      "<SOS> which are the best hollywood movies ? <EOS>\n",
      "<SOS> which is the all sentence best hollywood movie ? <EOS>\n",
      "-------------------\n",
      "################\n",
      "<SOS> which has more career opportunities biotechnology or biomedical science ? <EOS>\n",
      "<SOS> which is better ergonomics or biomedical skill ? <EOS>\n",
      "-------------------\n",
      "<SOS> why do people drink coffee ? <EOS>\n",
      "<SOS> why do so many mass enjoy drinking coffee ? <EOS>\n",
      "-------------------\n",
      "<SOS> what are the best novels in english ? <EOS>\n",
      "<SOS> what are the practiced novel in english ? and why ? <EOS>\n",
      "-------------------\n",
      "<SOS> which are the best hollywood movies ? <EOS>\n",
      "<SOS> which is the all clip best hollywood movie ? <EOS>\n",
      "-------------------\n",
      "################\n",
      "<SOS> which has more career opportunities biotechnology or biomedical science ? <EOS>\n",
      "<SOS> which is better biotechnology or biomedical skill ? <EOS>\n",
      "-------------------\n",
      "<SOS> why do people drink coffee ? <EOS>\n",
      "<SOS> why do so many people enjoy drinking java ? <EOS>\n",
      "-------------------\n",
      "<SOS> what are the best novels in english ? <EOS>\n",
      "<SOS> what are the best novels in side ? and why ? <EOS>\n",
      "-------------------\n",
      "<SOS> which are the best hollywood movies ? <EOS>\n",
      "<SOS> which is the all clip honest hollywood movie ? <EOS>\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# repeat = 3\n",
    "# max_count = 3\n",
    "\n",
    "# for _ in range(repeat):\n",
    "#     print('################')\n",
    "#     count = 0\n",
    "#     for seq1, seq2 in dataset:\n",
    "#         words1 =  [dataset.idx2word[idx.item()] for idx in seq1] \n",
    "#         words2 = [dataset.idx2word[idx.item()] for idx in seq2]\n",
    "#         print(' '.join(words1))\n",
    "#         print(' '.join(words2))\n",
    "#         print('-------------------')\n",
    "#         count += 1\n",
    "#         if count > max_count:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
