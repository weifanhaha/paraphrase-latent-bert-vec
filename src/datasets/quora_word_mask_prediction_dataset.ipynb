{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraWordMaskPredictDataset(Dataset):\n",
    "    def __init__(self, mode, train_size=5000, val_size=1000, test_size=1000, \n",
    "                 text_path='../data/quora_train.txt', pretrained_model_name=\"bert-large-cased-whole-word-masking\", \n",
    "                 topk=50, bow_strategy='simple_sum', indiv_topk=10, indiv_topp=0.01):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.topk = topk\n",
    "        self.bow_strategy = bow_strategy # simple_sum, mask_sum, indiv_topk, indiv_topp, indiv_neighbors\n",
    "        self.indiv_topk = indiv_topk\n",
    "        self.indiv_topp = indiv_topp\n",
    "        \n",
    "        self.tokenizer = self.init_tokenizer(pretrained_model_name)\n",
    "        self.mask_predict_model = AutoModelForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "        self.sentences = self.read_text(text_path)\n",
    "        self.init_constants()\n",
    "        \n",
    "        self.n_words = len(self.tokenizer)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mask_predict_model = self.mask_predict_model.to(self.device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        seq1, seq2 = sentence.split('\\t')\n",
    "        \n",
    "        tokens1 = self.tokenizer.tokenize(seq1)\n",
    "        word_pieces1 =  [self.SOS_token] + tokens1 + [self.EOS_token]\n",
    "        idxes1 = self.tokenizer.convert_tokens_to_ids(word_pieces1)\n",
    "        \n",
    "        tokens2 = self.tokenizer.tokenize(seq2)\n",
    "        word_pieces2 = [self.SOS_token] + tokens2 + [self.EOS_token]\n",
    "        idxes2 = self.tokenizer.convert_tokens_to_ids(word_pieces2)\n",
    "        \n",
    "        seq1_tensor = torch.tensor(idxes1, dtype=torch.long)        \n",
    "        seq2_tensor = torch.tensor(idxes2, dtype=torch.long)\n",
    "        \n",
    "        # pass the string sentence to mask words\n",
    "        seq1_predict_token_tensors = self.get_source_predict_tokens(seq1)\n",
    "\n",
    "        concat_tensor = torch.cat((seq1_tensor, seq1_predict_token_tensors.cpu()))\n",
    "        concat_tensor = torch.cat((concat_tensor, torch.tensor([self.EOS_token_id])))\n",
    "\n",
    "        return concat_tensor, seq2_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_size\n",
    "        elif self.mode == 'val':\n",
    "            return self.val_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    def get_source_predict_tokens(self, seq1):\n",
    "        mask_sentences = []\n",
    "        \n",
    "        seq1_words = seq1.split()\n",
    "        \n",
    "        for i in range(len(seq1_words)):\n",
    "            word = seq1_words[i]\n",
    "            seq1_words[i] = self.MASK_token\n",
    "            sentence = ' '.join(seq1_words)\n",
    "            tokens = self.tokenizer.tokenize(sentence)\n",
    "            word_pieces =  [self.SOS_token] + tokens + [self.EOS_token]\n",
    "            idxes = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "            mask_sentences.append(torch.tensor(idxes, dtype=torch.long))\n",
    "            seq1_words[i] = word\n",
    "\n",
    "        mask_stack = pad_sequence(mask_sentences, batch_first=True)\n",
    "        \n",
    "        masks_tensors = torch.zeros(mask_stack.shape,\n",
    "                                    dtype=torch.long)\n",
    "        # let bert attends only not padding ones\n",
    "        masks_tensors = masks_tensors.masked_fill(\n",
    "            mask_stack != 0, 1)\n",
    "\n",
    "        mask_stack = mask_stack.to(self.device)\n",
    "        masks_tensors = masks_tensors.to(self.device)\n",
    "        self.mask_predict_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = self.mask_predict_model(mask_stack, attention_mask=masks_tensors)[0]\n",
    "        pred = pred.cpu()\n",
    "            \n",
    "        if self.bow_strategy == 'simple_sum':\n",
    "            bows = torch.zeros(self.n_words)\n",
    "            for i in range(pred.shape[0]):\n",
    "                prob = pred[i][i+1]\n",
    "                bows += prob\n",
    "            _, indices = torch.topk(bows, self.topk)\n",
    "            return indices\n",
    "        elif self.bow_strategy == 'indiv_topk':\n",
    "            # todo: try to improve efficiency with matrix calculation\n",
    "            probs, indiv_indices = torch.topk(pred, self.indiv_topk)\n",
    "            bows = torch.zeros(self.n_words)\n",
    "            for i in range(indiv_indices.shape[0]):\n",
    "                prob, indices = probs[i][i+1], indiv_indices[i][i+1]\n",
    "                res = torch.zeros(self.n_words)\n",
    "                res = res.scatter(0, indices, prob)\n",
    "                bows += res\n",
    "            _, indices = torch.topk(bows, self.topk)\n",
    "            return indices    \n",
    "        elif self.bow_strategy == 'indiv_neighbors':\n",
    "            probs, indiv_indices = torch.topk(pred, self.indiv_topk)\n",
    "            final_indices = []\n",
    "            for i in range(indiv_indices.shape[0]):\n",
    "                _, indices = probs[i][i+1], indiv_indices[i][i+1]\n",
    "                final_indices.append(indices)\n",
    "            \n",
    "            return torch.cat(final_indices)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"bow strategy is not defined\")\n",
    "\n",
    "    def init_tokenizer(self, pretrained_model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking\")  \n",
    "        return tokenizer\n",
    "    \n",
    "    def init_constants(self):\n",
    "        PAD_id,  SOS_id, EOS_id, UNK_id = self.tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\"])\n",
    "        self.PAD_token_id = PAD_id\n",
    "        self.SOS_token_id = SOS_id\n",
    "        self.EOS_token_id = EOS_id\n",
    "        self.UNK_token_id = UNK_id\n",
    "        \n",
    "        self.PAD_token = '[PAD]'\n",
    "        self.SOS_token = '[CLS]'\n",
    "        self.EOS_token = '[SEP]'\n",
    "        self.UNK_token = '[UNK]'\n",
    "        \n",
    "        self.MASK_token = '[MASK]'\n",
    "        self.MASK_token_id = self.tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n",
    "    \n",
    "    def read_text(self, text_path):\n",
    "        # add words to dictionary\n",
    "        f = open(text_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        if self.mode == \"train\":\n",
    "            lines = lines[:self.train_size]\n",
    "        elif self.mode == 'val':\n",
    "            lines = lines[self.train_size:self.train_size+self.val_size]\n",
    "        else:\n",
    "            lines = lines[self.train_size+self.val_size:self.train_size+self.val_size+self.test_size]\n",
    "        \n",
    "        return lines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# dataset = QuoraWordMaskPredictDataset(\"train\", 1000, 100, text_path='../../data/quora_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mini_batch(samples):\n",
    "#     seq1_tensors = [s[0] for s in samples]\n",
    "#     seq2_tensors = [s[1] for s in samples]\n",
    "\n",
    "#     # zero pad\n",
    "#     seq1_tensors = pad_sequence(seq1_tensors,\n",
    "#                                   batch_first=True)\n",
    "\n",
    "#     seq2_tensors = pad_sequence(seq2_tensors,\n",
    "#                                   batch_first=True)    \n",
    "    \n",
    "#     return seq1_tensors, seq2_tensors\n",
    "\n",
    "\n",
    "# # it takes time to predict masked component\n",
    "# # to improve -> use gpu and calculate outside the dataset\n",
    "# data_loader = DataLoader(dataset, batch_size=64, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [00:02<00:35,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 87]) torch.Size([64, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 2/16 [00:04<00:32,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 74]) torch.Size([64, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 3/16 [00:06<00:28,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 82]) torch.Size([64, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 4/16 [00:08<00:25,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 81]) torch.Size([64, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███▏      | 5/16 [00:10<00:22,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 80]) torch.Size([64, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 6/16 [00:12<00:20,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 84]) torch.Size([64, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 7/16 [00:14<00:18,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 87]) torch.Size([64, 43])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 8/16 [00:16<00:16,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 81]) torch.Size([64, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▋    | 9/16 [00:18<00:14,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 83]) torch.Size([64, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 10/16 [00:20<00:12,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 77]) torch.Size([64, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 11/16 [00:22<00:10,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 95]) torch.Size([64, 44])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 12/16 [00:24<00:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 87]) torch.Size([64, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████▏ | 13/16 [00:26<00:06,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 85]) torch.Size([64, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 14/16 [00:29<00:04,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 78]) torch.Size([64, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 15/16 [00:31<00:02,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 88]) torch.Size([64, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:32<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 88]) torch.Size([40, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# for seq1, seq2 in tqdm(data_loader):\n",
    "# #     pass\n",
    "# #     print(seq1)\n",
    "#     print(seq1.shape, seq2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
