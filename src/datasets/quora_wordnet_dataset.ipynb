{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.corpus import wordnet \n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# for dictionary\n",
    "# from quora_dataset import QuoraDataset\n",
    "from nltk.corpus import stopwords\n",
    "# called from src\n",
    "from datasets.quora_dataset import QuoraDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraWordnetDataset(Dataset):\n",
    "    def __init__(self, mode, train_size=5000, val_size=1000, test_size=1000, text_path='../data/quora_train.txt',\n",
    "                 word2idx_path='../data/word2idx.npy', idx2word_path='../data/idx2word_path.npy',\n",
    "                 dic_sentences_num=150000, load_dic=True, indiv_k=5, topk=50, replace_origin=False, append_bow=True):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.indiv_k = indiv_k\n",
    "        self.topk = topk\n",
    "        self.replace_origin = replace_origin\n",
    "        self.append_bow = append_bow\n",
    "        self.sentences = []\n",
    "\n",
    "        self._init_constants()\n",
    "        self._init_sentences(text_path)\n",
    "        self.word2idx, self.idx2word = QuoraDataset.build_dictionary(\n",
    "            text_path=text_path, sentences_num=dic_sentences_num, load_dic=load_dic,\n",
    "            word2idx_path=word2idx_path, idx2word_path=idx2word_path\n",
    "        )\n",
    "        self.n_words = len(self.word2idx)\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        # set random seed ?\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq1, seq2 = self.sentences[idx]\n",
    "        \n",
    "        idxes1 = [self._get_index(word) for word in seq1.split(' ')]\n",
    "        idxes1 = [self.SOS_token_id] + idxes1 + [self.EOS_token_id]        \n",
    "\n",
    "        if self.replace_origin:\n",
    "            idxes1 = self.get_replaced_sentence_idxes(seq1)\n",
    "        \n",
    "        elif self.append_bow:\n",
    "            # append wordnet synonyms\n",
    "            wordnet_idxes = self.get_wordnet_idxes(seq1)\n",
    "            idxes1 = idxes1 + wordnet_idxes + [self.EOS_token_id]\n",
    "                    \n",
    "        idxes2 = [self._get_index(word) for word in seq2.split(' ')]\n",
    "        idxes2 = [self.SOS_token_id] + idxes2 + [self.EOS_token_id]\n",
    "\n",
    "        seq1_tensor = torch.tensor(idxes1, dtype=torch.long)        \n",
    "        seq2_tensor = torch.tensor(idxes2, dtype=torch.long)\n",
    "        \n",
    "        return (seq1_tensor, seq2_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_size\n",
    "        elif self.mode == 'val':\n",
    "            return self.val_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "    \n",
    "    def _get_index(self, word):\n",
    "        try: \n",
    "            index = self.word2idx[word]\n",
    "        except KeyError:\n",
    "            index = self.UNK_token_id\n",
    "        return index\n",
    "\n",
    "    def _init_constants(self):\n",
    "        self.PAD_token = '<PAD>'\n",
    "        self.SOS_token = '<SOS>'\n",
    "        self.EOS_token = '<EOS>'\n",
    "        self.UNK_token = '<UNK>'\n",
    "        self.PAD_token_id = 0\n",
    "        self.SOS_token_id = 1\n",
    "        self.EOS_token_id = 2\n",
    "        self.UNK_token_id = 3\n",
    "    \n",
    "    def _init_sentences(self, text_path):\n",
    "        f = open(text_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        # shuffle\n",
    "        np.random.shuffle(lines)\n",
    "        if self.mode == \"train\":\n",
    "            lines = lines[:self.train_size]\n",
    "        elif self.mode == 'val':\n",
    "            lines = lines[self.train_size:self.train_size+self.val_size]\n",
    "        else:\n",
    "            lines = lines[self.train_size+self.val_size:self.train_size+self.val_size+self.test_size]\n",
    "        \n",
    "        def normalize_sentence(s):\n",
    "            s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "            s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "            return s\n",
    "\n",
    "        for line in tqdm(lines):\n",
    "            seq1, seq2 = [normalize_sentence(seq) for seq in line.split('\\t')]\n",
    "            self.sentences.append((seq1, seq2))\n",
    "\n",
    "    def get_wordnet_idxes(self, seq):\n",
    "        indiv_k = self.indiv_k\n",
    "        words = seq.split()\n",
    "        wordnet_idxes = []\n",
    "\n",
    "        for word in words:\n",
    "            syn_set = self._get_synset(word)\n",
    "            \n",
    "            # need not to append\n",
    "            if len(syn_set) == 0:\n",
    "                continue\n",
    "\n",
    "            if len(syn_set) > indiv_k:\n",
    "                syn_set = random.sample(syn_set, indiv_k)\n",
    "\n",
    "            for syn in syn_set:\n",
    "                if self._get_index(syn) != self.UNK_token_id:\n",
    "                    wordnet_idxes.append(self._get_index(syn))\n",
    "            if len(wordnet_idxes) > self.topk:\n",
    "                wordnet_idxes = random.sample(wordnet_idxes, self.topk)            \n",
    "        return wordnet_idxes\n",
    "\n",
    "    \n",
    "    # to be implement\n",
    "    def get_replaced_sentence_idxes(self, seq):\n",
    "        indiv_k = self.indiv_k\n",
    "        words = seq.split()\n",
    "        new_sentence_idxes = []\n",
    "        wordnet_idxes = []\n",
    "        \n",
    "        for word in words:\n",
    "            syn_set = self._get_synset(word)\n",
    "            \n",
    "            if word in self.stopwords or len(syn_set) == 0:\n",
    "                new_sentence_idxes.append(self._get_index(word))\n",
    "                continue\n",
    "            \n",
    "            sampled_syn = random.sample(syn_set, 1)[0]\n",
    "            if self._get_index(sampled_syn) != self.UNK_token_id:\n",
    "                new_sentence_idxes.append(self._get_index(sampled_syn))\n",
    "                # append the original word to the bow\n",
    "                wordnet_idxes.append(self._get_index(word))\n",
    "            else:\n",
    "                new_sentence_idxes.append(self._get_index(word))\n",
    "\n",
    "            if len(syn_set) > indiv_k:\n",
    "                syn_set = random.sample(syn_set, indiv_k)\n",
    "\n",
    "            for syn in syn_set:\n",
    "                if self._get_index(syn) != self.UNK_token_id:\n",
    "                    wordnet_idxes.append(self._get_index(syn))\n",
    "            if len(wordnet_idxes) > self.topk:\n",
    "                wordnet_idxes = random.sample(wordnet_idxes, self.topk)\n",
    "                \n",
    "        if self.append_bow:\n",
    "            return [self.SOS_token_id] + new_sentence_idxes + [self.EOS_token_id] + wordnet_idxes + [self.EOS_token_id]\n",
    "        else:\n",
    "            return [self.SOS_token_id] + new_sentence_idxes + [self.EOS_token_id]\n",
    "    \n",
    "    def _get_synset(self, word):\n",
    "        syn_set = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas(): \n",
    "                # ignore the phrase\n",
    "                if '_' in l.name() or '-' in l.name():\n",
    "                    continue\n",
    "                syn_set.add(l.name()) \n",
    "        if word in syn_set:\n",
    "            syn_set.remove(word)\n",
    "        return syn_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 63171.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loading the Dictionary...\n",
      "[Info] Dictionary Loaded\n"
     ]
    }
   ],
   "source": [
    "# dataset = QuoraWordnetDataset(\"train\", 50000, 1000, text_path='../../data/quora_train.txt', load_dic=True,\n",
    "# word2idx_path='../../data/word2idx.npy', idx2word_path='../../data/idx2word_path.npy', replace_origin=True, append_bow=False)\n",
    "# # # dataset = QuoraDataset(\"train\", 50000, 1000, load_dic=False)\n",
    "# # word2idx, idx2word = QuoraDataset.build_dictionary(sentences_num=100000, load_dic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  3862, 13426,     6,     7,     8,     9,    10, 23729,    12,\n",
      "          3576,    14,    15,    15,    16,    17,    18,  1535,    20,    21,\n",
      "            22,     2,     4,  3862,     5, 12876,   109, 13426, 28192,  9491,\n",
      "          2110, 30648,    13,  4606,  8843,    11, 35078, 23729,    25,    13,\n",
      "          3576,  8843, 27701,  7117,  1157, 33375,  3835,  7959,    19, 30099,\n",
      "          9631,    40,     2],\n",
      "        [    1,    31,    32, 11190,    33,     7, 24615,    35,    22,     2,\n",
      "             5,   109, 13426, 12876,    34, 17889,  1682,   184,  1610,  1399,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    1,    31,    37, 11190,  5092,    12,    45,    42,    43, 23435,\n",
      "            22,     2,     5, 13426,   679, 12876,    40,  7278,    19,   889,\n",
      "          1969,  5092,    41,  3440,  2972,    45,  5336,    44, 13414, 15698,\n",
      "         23435,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    1,    28,    32,   492,    53,  3244,    38,  1422,    22,     2,\n",
      "            48,  2675,  5000,  9589,   105,    49, 34290,    53,    50,  7261,\n",
      "         21441,    98,    51,  6294,    40,  4626,  4574,   503,     2,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    1,    28,    54,    55,  2524, 19598,   435,  6033,    22,     2,\n",
      "            56,   482, 34806,  2524, 26085,    57, 19598,    58,  7832,   422,\n",
      "           680,   435,    45,    59,   585,  4620, 24286, 19792,     2,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    1,    28,    60,     7, 16371,  4234, 16455,    64,   601, 12730,\n",
      "          7343,    68, 19668,    70,    71, 12004,    73,    22,     2,    61,\n",
      "          1976,  6974,   168,    62,  4234,    63, 10737, 12312,  1694,    65,\n",
      "           601, 11931,    66,  3989,  4874, 12730,    67,   336,  1993, 29420,\n",
      "          5671, 12462,    69, 19668, 25116,  1686,    72,  9404, 12004,     2,\n",
      "             0,     0,     0]]) tensor([[ 1,  5, 23,  7, 24,  8,  9, 25, 12, 26, 27,  8, 28, 17, 29, 19, 20, 21,\n",
      "         22, 30,  2],\n",
      "        [ 1, 28, 36,  5, 37, 38, 33,  7, 39, 35, 22, 30,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 1, 31, 32,  5, 45, 46, 42, 47, 44, 22, 30,  2,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 1, 31, 32, 52, 48, 53, 50, 38, 51, 22, 30,  2,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 1, 28, 54, 55, 56, 57, 58, 22, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 1, 31, 74,  7, 61, 62, 75, 76, 69, 77, 27, 78, 79, 80, 38, 81, 27, 78,\n",
      "         22, 30,  2]])\n"
     ]
    }
   ],
   "source": [
    "# def create_mini_batch(samples):\n",
    "#     seq1_tensors = [s[0] for s in samples]\n",
    "#     seq2_tensors = [s[1] for s in samples]\n",
    "\n",
    "#     # zero pad\n",
    "#     seq1_tensors = pad_sequence(seq1_tensors,\n",
    "#                                   batch_first=True)\n",
    "\n",
    "#     seq2_tensors = pad_sequence(seq2_tensors,\n",
    "#                                   batch_first=True)    \n",
    "    \n",
    "#     return seq1_tensors, seq2_tensors\n",
    "\n",
    "\n",
    "# # dataset = QuoraDataset(\"train\", 50000, 1000, load_dic=False)\n",
    "# data_loader = DataLoader(dataset, batch_size=6, collate_fn=create_mini_batch)\n",
    "# seq1, seq2 = next(iter(data_loader))\n",
    "# print(seq1, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How\n",
      "set()\n",
      "do\n",
      "{'practice', 'set', 'fare', 'perform', 'answer', 'act', 'bash', 'cause', 'behave', 'ut', 'manage', 'coiffe', 'exercise', 'practise', 'coif', 'arrange', 'serve', 'coiffure', 'make', 'dress', 'DO', 'come', 'execute', 'suffice', 'brawl', 'doh'}\n",
      "''do'' is stopword\n",
      "you\n",
      "set()\n",
      "''you'' is stopword\n",
      "speak\n",
      "{'verbalise', 'mouth', 'verbalize', 'address', 'talk', 'utter'}\n",
      "in\n",
      "{'Indiana', 'inch', 'indium', 'In', 'inwards', 'inward', 'IN'}\n",
      "''in'' is stopword\n",
      "front\n",
      "{'forepart', 'figurehead', 'presence', 'movement', 'face', 'look', 'battlefront', 'breast', 'strawman'}\n",
      "of\n",
      "set()\n",
      "''of'' is stopword\n",
      "large\n",
      "{'orotund', 'enceinte', 'great', 'tumid', 'expectant', 'turgid', 'boastfully', 'bombastic', 'gravid', 'heavy', 'magnanimous', 'vauntingly', 'big', 'prominent', 'declamatory'}\n",
      "groups\n",
      "{'radical', 'group', 'aggroup', 'grouping'}\n",
      "of\n",
      "set()\n",
      "''of'' is stopword\n",
      "people\n",
      "{'mass', 'multitude', 'citizenry', 'masses'}\n",
      "?\n",
      "set()\n",
      "['utter', 'mouth', 'verbalise', 'talk', 'verbalize', 'speak', 'battlefront', 'look', 'face', 'movement', 'breast', 'front', 'bombastic', 'heavy', 'turgid', 'big', 'tumid', 'large', 'radical', 'group', 'aggroup', 'grouping', 'groups', 'mass', 'multitude', 'citizenry', 'masses', 'people']\n",
      "['How', 'do', 'you', 'address', 'in', 'look', 'of', 'turgid', 'group', 'of', 'citizenry', '?']\n"
     ]
    }
   ],
   "source": [
    "# # analyze\n",
    "# # sentence = 'What is the food you can eat every day for breakfast lunch and dinner ?'\n",
    "# sentence = 'How do you speak in front of large groups of people ?'\n",
    "\n",
    "\n",
    "# words = sentence.split()\n",
    "\n",
    "# stopws =  stopwords.words('english')\n",
    "\n",
    "# # indiv_k\n",
    "# indiv_k = 5\n",
    "# appends = []\n",
    "\n",
    "# new_sentence = []\n",
    "\n",
    "# for word in words:\n",
    "#     print(word)\n",
    "    \n",
    "#     syn_set = set()\n",
    "    \n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         for l in syn.lemmas(): \n",
    "#             # ignore the phrase\n",
    "#             if '_' in l.name() or '-' in l.name():\n",
    "#                 continue\n",
    "#             syn_set.add(l.name()) \n",
    "#     if word in syn_set:\n",
    "#         syn_set.remove(word)\n",
    "#     print(syn_set)\n",
    "    \n",
    "#     # need not to append\n",
    "#     if word in stopws or len(syn_set) == 0:\n",
    "#         if word in stopws:\n",
    "#             print(\"''{}'' is stopword\".format(word))\n",
    "#         new_sentence.append(word)\n",
    "#         continue\n",
    "        \n",
    "#     sampled_syn = random.sample(syn_set, 1)[0]\n",
    "#     new_sentence.append(sampled_syn)\n",
    "#     if len(syn_set) > indiv_k:\n",
    "#         syn_set = random.sample(syn_set, indiv_k)\n",
    "\n",
    "#     for syn in syn_set:\n",
    "#         appends.append(syn)\n",
    "#     # append original word\n",
    "#     appends.append(word)\n",
    "# print(appends)\n",
    "# print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
