{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.Models import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "# from transformer.Models import Transformer, Encoder, Decoder\n",
    "# from datasets.quora_bert_mask_predict_prob_dataset import QuoraBertMaskPredictProbDataset\n",
    "from transformer.Optim import ScheduledOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import os\n",
    "import random\n",
    "\n",
    "class QuoraBertMaskPredictProbDataset(Dataset):\n",
    "    def __init__(self, mode, train_size=5000, val_size=1000, test_size=1000, \n",
    "                 text_path='../data/quora_train.txt', pretrained_model_name='bert-base-cased', preprocessed_folder=None):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.tokenizer = self.init_tokenizer(pretrained_model_name)\n",
    "        self.mask_predict_model = BertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "        self.sentences = self.read_text(text_path)\n",
    "        self.init_constants()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        self.n_words = len(self.tokenizer)\n",
    "        \n",
    "        if preprocessed_folder is not None and os.path.exists(preprocessed_folder):\n",
    "            self.preprocessed_folder = preprocessed_folder\n",
    "        else:\n",
    "            self.preprocessed_folder = None\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.mask_predict_model = self.mask_predict_model.to(self.device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        seq1, seq2 = sentence.split('\\t')\n",
    "        \n",
    "        tokens1 = self.tokenizer.tokenize(seq1)\n",
    "        word_pieces1 =  [self.SOS_token] + tokens1 + [self.EOS_token]\n",
    "        idxes1 = self.tokenizer.convert_tokens_to_ids(word_pieces1)\n",
    "        \n",
    "        tokens2 = self.tokenizer.tokenize(seq2)\n",
    "        word_pieces2 = [self.SOS_token] + tokens2 + [self.EOS_token]\n",
    "        idxes2 = self.tokenizer.convert_tokens_to_ids(word_pieces2)\n",
    "        \n",
    "        seq1_tensor = torch.tensor(idxes1, dtype=torch.long)\n",
    "        seq2_tensor = torch.tensor(idxes2, dtype=torch.long)\n",
    "        \n",
    "        if self.preprocessed_folder is not None:\n",
    "            if self.mode == 'train':\n",
    "                pretrained_idx = idx\n",
    "            elif self.mode == 'val':\n",
    "                pretrained_idx = self.train_size + idx\n",
    "            else:\n",
    "                pretrained_idx = self.train_size + self.val_size + idx\n",
    "            preprocessed_file = \"{}/{}.npy\".format(self.preprocessed_folder, pretrained_idx)\n",
    "            mask_probs = np.load(preprocessed_file, allow_pickle=True)\n",
    "            mask_probs = torch.tensor(mask_probs)\n",
    "        else:\n",
    "            # mask each word from the begining to the end\n",
    "            # get the probability distribution of the mask tokens\n",
    "            pred = self.get_mask_pred_probs(seq1_tensor)\n",
    "            mask_probs = self.get_stack_probs(pred)\n",
    "        \n",
    "        return seq1_tensor, seq2_tensor, mask_probs\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_size\n",
    "        elif self.mode == 'val':\n",
    "            return self.val_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    # [CLS]  [M]  w2  w3  [SEP]        \n",
    "    # [CLS]  w1  [M]  w3  [SEP]        \n",
    "    # [CLS]  w1 w2  [M]  [SEP]        \n",
    "    # get the probability of [M] for each mask-prediction case\n",
    "    # mask for (number of words) times and every time get (number of words + 2) probability\n",
    "    # TODO: get the pred for only once\n",
    "    def get_mask_pred_probs(self, seq1):\n",
    "        mask_sentences = []\n",
    "        \n",
    "        for i in range(1, len(seq1) - 1):\n",
    "            mask_seq = seq1.detach().clone()\n",
    "            mask_seq[i] = self.MASK_token_id\n",
    "            mask_sentences.append(mask_seq)\n",
    "\n",
    "        mask_stack = torch.stack(mask_sentences)\n",
    "        mask_stack = mask_stack.to(self.device)\n",
    "        \n",
    "        self.mask_predict_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = self.mask_predict_model(mask_stack)[0]\n",
    "        pred = pred.cpu()\n",
    "        return pred\n",
    "    \n",
    "    def get_stack_probs(self, pred):\n",
    "        bos_prob = torch.zeros(self.n_words)\n",
    "        bos_prob[self.SOS_token_id] = 1\n",
    "\n",
    "        eos_prob = torch.zeros(self.n_words)\n",
    "        eos_prob[self.EOS_token_id] = 1\n",
    "\n",
    "        mask_preds = []\n",
    "        for idx in range(pred.shape[0]):\n",
    "            mask_preds.append(pred[idx][idx+1])\n",
    "        mask_stack = torch.stack(mask_preds)\n",
    "        mask_stack = torch.cat((bos_prob.reshape(1,-1), mask_stack, eos_prob.reshape(1,-1)))\n",
    "        return self.softmax(mask_stack)\n",
    "    \n",
    "    def init_tokenizer(self, pretrained_model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    def init_constants(self):\n",
    "        PAD_id,  SOS_id, EOS_id, UNK_id = self.tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\"])\n",
    "        self.PAD_token_id = PAD_id\n",
    "        self.SOS_token_id = SOS_id\n",
    "        self.EOS_token_id = EOS_id\n",
    "        self.UNK_token_id = UNK_id\n",
    "        \n",
    "        self.PAD_token = '[PAD]'\n",
    "        self.SOS_token = '[CLS]'\n",
    "        self.EOS_token = '[SEP]'\n",
    "        self.UNK_token = '[UNK]'\n",
    "        \n",
    "        self.MASK_token = '[MASK]'\n",
    "        self.MASK_token_id = self.tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n",
    "\n",
    "        \n",
    "    def read_text(self, text_path):\n",
    "        # add words to dictionary\n",
    "        f = open(text_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        # shuffle\n",
    "        np.random.shuffle(lines)\n",
    "        if self.mode == \"train\":\n",
    "            lines = lines[:self.train_size]\n",
    "        elif self.mode == 'val':\n",
    "            lines = lines[self.train_size:self.train_size+self.val_size]\n",
    "        else:\n",
    "            lines = lines[self.train_size+self.val_size:self.train_size+self.val_size+self.test_size]\n",
    "        \n",
    "        return lines\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "\n",
    "# fix seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_model_path': '../models/transformer_bert_enc_attention.pth', 'log_file': '../logs/transformer_bert_enc_attention.txt', 'test_output_file': '../outputs/test_transformer_bert_enc_attention.txt', 'val_output_file': '../outputs/val_transformer_bert_enc_attention.txt', 'preprocessed_folder': '../data/preprocess_quora_bert_mask_predict/', 'dataset': 'quora_bert_dataset', 'num_epochs': 50, 'batch_size': 100, 'd_model': 450, 'd_inner_hid': 512, 'd_k': 50, 'd_v': 50, 'n_head': 9, 'n_layers': 3, 'n_warmup_steps': 12000, 'dropout': 0.1, 'embs_share_weight': True, 'proj_share_weight': True, 'label_smoothing': False, 'train_size': 100000, 'val_size': 4000, 'test_size': 20000, 'is_bow': False, 'lr': '1e-3'}\n"
     ]
    }
   ],
   "source": [
    "##### Read Arguments from Config File #####\n",
    "\n",
    "config_path = '../../configs/dpng_transformer_bert_attention.yaml'\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(config)\n",
    "\n",
    "# save_model_path = config['save_model_path']\n",
    "# log_file = config['log_file']\n",
    "# preprocessed_folder = config['preprocessed_folder']\n",
    "save_model_path = '../../models/tune/transformer_key_enc_bert_val_attention_alpha0.5_softmax.pth'\n",
    "log_file = '../../logs/tune/transformer_key_enc_bert_val_attention_alpha0.5_softmax.txt'\n",
    "output_file = '../../outputs/tune/transformer_key_enc_bert_val_attention_alpha0.5_softmax.txt'\n",
    "\n",
    "\n",
    "# set model and log path\n",
    "seed_model_root = '../../models/fixseed/seed{}/'.format(seed)\n",
    "seed_log_root = '../../logs/fixseed/seed{}/'.format(seed)\n",
    "seed_output_root = '../../outputs/fixseed/seed{}/'.format(seed)\n",
    "if not os.path.exists(seed_model_root):\n",
    "    os.makedirs(seed_model_root)\n",
    "\n",
    "if not os.path.exists(seed_log_root):\n",
    "    os.makedirs(seed_log_root)\n",
    "\n",
    "save_model_path = seed_model_root + save_model_path.split('/')[-1]\n",
    "log_file = seed_log_root + log_file.split('/')[-1]\n",
    "output_file = seed_output_root + output_file.split('/')[-1]\n",
    "print('seed: ', seed)\n",
    "print('save model path: ', save_model_path)\n",
    "print('log path: ', log_file)\n",
    "print('output path: ', output_file)\n",
    "\n",
    "num_epochs = config['num_epochs']\n",
    "# batch_size = config['batch_size']\n",
    "batch_size = 80\n",
    "\n",
    "\n",
    "\n",
    "d_model = config['d_model']\n",
    "d_inner_hid = config['d_inner_hid']\n",
    "d_k = config['d_k']\n",
    "d_v = config['d_v']\n",
    "\n",
    "n_head = config['n_head']\n",
    "n_layers = config['n_layers']\n",
    "n_warmup_steps = config['n_warmup_steps']\n",
    "\n",
    "dropout = config['dropout']\n",
    "embs_share_weight = config['embs_share_weight']\n",
    "proj_share_weight = config['proj_share_weight']\n",
    "label_smoothing = config['label_smoothing']\n",
    "\n",
    "train_size = config['train_size']\n",
    "val_size = config['val_size']\n",
    "test_size = config['test_size']\n",
    "\n",
    "lr = float(config['lr'])\n",
    "# lr = 5e-4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_alpha = 0 # normal\n",
    "bert_alpha = 0.5 # half-half\n",
    "# bert_alpha = 1 # only bert attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149263\n"
     ]
    }
   ],
   "source": [
    "def create_mini_batch(samples):\n",
    "    seq1_tensors = [s[0] for s in samples]\n",
    "    seq2_tensors = [s[1] for s in samples]\n",
    "    probs_tensors = [s[2] for s in samples]\n",
    "\n",
    "    # zero pad\n",
    "    seq1_tensors = pad_sequence(seq1_tensors,\n",
    "                                  batch_first=True)\n",
    "\n",
    "    seq2_tensors = pad_sequence(seq2_tensors,\n",
    "                                  batch_first=True)\n",
    "    probs_tensors = pad_sequence(probs_tensors,\n",
    "                                  batch_first=True)\n",
    "    \n",
    "    return seq1_tensors, seq2_tensors, probs_tensors\n",
    "\n",
    "dataset = QuoraBertMaskPredictProbDataset(\"train\", train_size, val_size, text_path='../../data/quora_train.txt')\n",
    "val_dataset = QuoraBertMaskPredictProbDataset(\"val\", train_size, val_size, text_path='../../data/quora_train.txt')\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer.SubLayers import MultiHeadAttention, PositionwiseFeedForward\n",
    "from transformer.Models import get_pad_mask, get_subsequent_mask\n",
    "from transformer.Models import PositionalEncoding, Encoder\n",
    "\n",
    "# QKV: dec_out  enc_out  bert_out\n",
    "class DecoderLayer(nn.Module):\n",
    "    ''' Compose with three layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1, bert_alpha=0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.bert_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "        self.bert_alpha = bert_alpha\n",
    "\n",
    "    def forward(\n",
    "            self, dec_input, enc_output, bert_out,\n",
    "            slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        \n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        \n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        \n",
    "        dec_bert_output, dec_enc_bert_attn = self.bert_attn(\n",
    "            dec_output, enc_output, bert_out, mask=dec_enc_attn_mask)\n",
    "        \n",
    "        dec_output = (1-self.bert_alpha) *  dec_output + self.bert_alpha * dec_bert_output\n",
    "        \n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' A decoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, n_position=200, dropout=0.1, bert_alpha=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout, bert_alpha=bert_alpha)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, bert_out, src_mask, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq)))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "        \n",
    "#         print('encoder output', enc_output.shape)\n",
    "#         print('decoder output', dec_output.shape)\n",
    "#         print('bert output', bert_out.shape)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, bert_out, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "            d_word_vec=512, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n",
    "            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True, bert_alpha=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "        \n",
    "        self.probnn = nn.Linear(n_src_vocab, d_model)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=src_pad_idx, dropout=dropout)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=trg_pad_idx, dropout=dropout, bert_alpha=bert_alpha)\n",
    "\n",
    "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        self.x_logit_scale = 1.\n",
    "        if trg_emb_prj_weight_sharing:\n",
    "            # Share the weight between target word embedding & last dense layer\n",
    "            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n",
    "            self.x_logit_scale = (d_model ** -0.5)\n",
    "\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n",
    "\n",
    "\n",
    "    def forward(self, src_seq, trg_seq, bert_prob, return_attns=False):\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_output, enc_slf_attn_list = self.encoder(src_seq, src_mask, return_attns=True)\n",
    "        \n",
    "        bert_out = self.probnn(bert_prob)\n",
    "#         print(\"bert prod:\", bert_prob.shape)\n",
    "#         print(\"(trans) bert out: \", bert_out.shape)\n",
    "        \n",
    "        dec_output, dec_slf_attn_list, dec_enc_attn_list = self.decoder(trg_seq, trg_mask, enc_output, bert_out, src_mask, return_attns=True)\n",
    "        seq_logit = self.trg_word_prj(dec_output) * self.x_logit_scale\n",
    "\n",
    "        if return_attns:\n",
    "            return seq_logit.view(-1, seq_logit.size(2)), enc_slf_attn_list, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return seq_logit.view(-1, seq_logit.size(2))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    dataset.n_words,\n",
    "    dataset.n_words,\n",
    "    src_pad_idx=dataset.PAD_token_id,\n",
    "    trg_pad_idx=dataset.PAD_token_id,\n",
    "    trg_emb_prj_weight_sharing=proj_share_weight,\n",
    "    emb_src_trg_weight_sharing=embs_share_weight,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    d_model=d_model,\n",
    "    d_word_vec=d_model,\n",
    "    d_inner=d_inner_hid,\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout,\n",
    "    bert_alpha=bert_alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09, lr=lr),\n",
    "    2.0, d_model, n_warmup_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train epoch\n",
    "def train_epoch(model, data_loader, optimizer, device, smoothing=False):\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "    \n",
    "    trange = tqdm(data_loader)\n",
    "\n",
    "    for seq1, seq2, probs, in trange:\n",
    "        src_seq = seq1.to(device)\n",
    "        trg_seq = seq2[:, :-1].to(device)\n",
    "        probs = probs.to(device)\n",
    "        gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq, probs)\n",
    "\n",
    "        loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, dataset.PAD_token_id, smoothing) \n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "        trange.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': n_correct/n_word\n",
    "        })\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_data_loader, device):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2, probs in tqdm(val_data_loader):\n",
    "\n",
    "            src_seq = seq1.to(device)\n",
    "            trg_seq = seq2[:, :-1].to(device)\n",
    "            probs = probs.to(device)\n",
    "            gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n",
    "            pred = model(src_seq, trg_seq, probs)\n",
    "\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, dataset.PAD_token_id, smoothing=False) \n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_performances(header, loss, accu, start_time, file):\n",
    "    print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n",
    "        'elapse: {elapse:3.3f} sec'.format(\n",
    "            header=f\"({header})\", ppl=math.exp(min(loss, 100)),\n",
    "            accu=100*accu, elapse=(time.time()-start_time))) \n",
    "    file.write(\n",
    "          '- {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n",
    "        'elapse: {elapse:3.3f} sec\\n'.format(\n",
    "            header=f\"({header})\", ppl=math.exp(min(loss, 100)),\n",
    "            accu=100*accu, elapse=(time.time()-start_time))\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:03<1:05:22,  3.93s/it, loss=1.36e+4, acc=0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 11.91 GiB total capacity; 3.40 GiB already allocated; 293.50 MiB free; 3.58 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-affdafef88b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} / {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mlog_performances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f0a4966059b5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, device, smoothing)\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss, n_correct, n_word = cal_performance(\n\u001b[1;32m     18\u001b[0m             pred, gold, dataset.PAD_token_id, smoothing) \n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_update_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 11.91 GiB total capacity; 3.40 GiB already allocated; 293.50 MiB free; 3.58 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "# log_file = \"../logs/tmp.txt\"\n",
    "f = open(log_file, 'w')\n",
    "best_loss = 999\n",
    "\n",
    "f.write(\"Config: {}\\n\".format(config))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {} / {}\".format(epoch + 1, num_epochs))\n",
    "    start = time.time()\n",
    "    train_loss, train_accu = train_epoch(model, data_loader, optimizer, device, smoothing=label_smoothing)\n",
    "    log_performances('Training', train_loss, train_accu, start, f)\n",
    "\n",
    "    start = time.time()\n",
    "    valid_loss, valid_accu = eval_epoch(model, val_data_loader, device)\n",
    "    log_performances('Validation', valid_loss, valid_accu, start, f)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        best_loss = valid_loss\n",
    "        print(\"model saved in Epoch {}\".format(epoch + 1))\n",
    "        f.write(\"model saved in Epoch {}\\n\".format(epoch + 1))\n",
    "        f.flush()\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.CustomModels import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class QuoraTextDataset(Dataset):\n",
    "    def __init__(self, mode, train_size=5000, val_size=1000, test_size=1000, text_path='../data/quora_train.txt'):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.sentences = []\n",
    "\n",
    "        self._init_sentences(text_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "        seq1, seq2 = self.sentences[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_size\n",
    "        elif self.mode == 'val':\n",
    "            return self.val_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "    \n",
    "    def _init_sentences(self, text_path):\n",
    "        f = open(text_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        np.random.shuffle(lines)\n",
    "        if self.mode == \"train\":\n",
    "            lines = lines[:self.train_size]\n",
    "        elif self.mode == 'val':\n",
    "            lines = lines[self.train_size:self.train_size+self.val_size]\n",
    "        else:\n",
    "            lines = lines[self.train_size+self.val_size:self.train_size+self.val_size+self.test_size]\n",
    "        \n",
    "        def normalize_sentence(s):\n",
    "            s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "            s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "            return s\n",
    "\n",
    "        for line in lines:\n",
    "            seq1, seq2 = [normalize_sentence(seq) for seq in line.split('\\t')]\n",
    "            self.sentences.append((seq1, seq2))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149263\n"
     ]
    }
   ],
   "source": [
    "tbatch_size = 1\n",
    "beam_size = 3\n",
    "max_seq_len = 30\n",
    "\n",
    "same_seeds(seed)\n",
    "# dataset = QuoraBertMaskPredictProbDataset(\"test\", train_size, val_size, test_size, preprocessed_folder=preprocessed_folder)\n",
    "test_dataset = QuoraBertMaskPredictProbDataset(\"test\", train_size, val_size, test_size, text_path='../../data/quora_train.txt')\n",
    "\n",
    "same_seeds(seed)\n",
    "text_dataset = QuoraTextDataset(\"test\", train_size, val_size, test_size, text_path='../../data/quora_train.txt')\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=tbatch_size, collate_fn=create_mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict((torch.load(\n",
    "        save_model_path, map_location=device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = dataset.PAD_token_id\n",
    "trg_pad_idx = dataset.PAD_token_id\n",
    "    \n",
    "trg_bos_idx = dataset.SOS_token_id\n",
    "trg_eos_idx = dataset.EOS_token_id\n",
    "unk_idx = dataset.UNK_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "        model=model,\n",
    "        beam_size=beam_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        trg_bos_idx=trg_bos_idx,\n",
    "        trg_eos_idx=trg_eos_idx,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'What', 'is', 'the', 'best', 'food', 'I', 'can', 'eat', 'every', 'day', 'for', 'breakfast', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# dry-run\n",
    "seq1, seq2, bert_prob = next(iter(test_data_loader))\n",
    "input_seq = seq1.to(device)\n",
    "bert_prob = bert_prob.to(device)\n",
    "pred_seq = translator.translate_sentence(input_seq, bert_prob)\n",
    "print(dataset.tokenizer.convert_ids_to_tokens(pred_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [37:29<00:00,  8.89it/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = dataset.tokenizer\n",
    "# count = 0\n",
    "with open(output_file, 'w') as f:\n",
    "    for i, (seq1, seq2, bert_prob) in enumerate(tqdm(test_data_loader)):\n",
    "        src_line, trg_line = text_dataset.sentences[i]\n",
    "\n",
    "        input_seq = seq1.to(device)\n",
    "        bert_prob = bert_prob.to(device)\n",
    "        trg_seq = seq2[0].tolist()\n",
    "\n",
    "        pred_seq = translator.translate_sentence(input_seq, bert_prob)\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_seq)\n",
    "        pred_line = tokenizer.convert_tokens_to_string(pred_tokens)\n",
    "        pred_line = pred_line.replace(dataset.SOS_token, '').replace(dataset.EOS_token, '')\n",
    "\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_seq[0].tolist())\n",
    "        input_line = tokenizer.convert_tokens_to_string(input_tokens)\n",
    "\n",
    "        trg_tokens = tokenizer.convert_ids_to_tokens(trg_seq)\n",
    "        trg_line = tokenizer.convert_tokens_to_string(trg_tokens)\n",
    "        trg_line = trg_line.replace(dataset.SOS_token, '').replace(dataset.EOS_token, '')\n",
    "\n",
    "\n",
    "#         print('*' * 80)\n",
    "#         print('Source: ', src_line.strip())\n",
    "#         print('Target: ', trg_line.strip())\n",
    "#         print('Input: ', input_line.strip())\n",
    "#         print('Predict: ', pred_line.strip())\n",
    "        \n",
    "        f.write('*' * 80)\n",
    "        f.write('\\n')\n",
    "        f.write('Source: {}\\n'.format(src_line.strip()))\n",
    "        f.write('Target: {}\\n'.format(trg_line.strip()))\n",
    "        f.write('Input: {}\\n'.format(input_line.strip()))\n",
    "        f.write('Predict: {}\\n'.format(pred_line.strip()))\n",
    "        f.flush()\n",
    "\n",
    "#         count += 1\n",
    "#         if count > 5:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
