{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "import os\n",
    "\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Optim import ScheduledOptim\n",
    "from utils import cal_loss, cal_performance, log_performances, same_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config_path CONFIG_PATH] [--preprocessed]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /shared_home/r08922168/.local/share/jupyter/runtime/kernel-043a19fb-de14-431e-a14f-359f6d7484a6.json\n",
      "usage: ipykernel_launcher.py [-h] [--config_path CONFIG_PATH] [--preprocessed]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /shared_home/r08922168/.local/share/jupyter/runtime/kernel-043a19fb-de14-431e-a14f-359f6d7484a6.json\n",
      "usage: ipykernel_launcher.py [-h] [--config_path CONFIG_PATH] [--preprocessed]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /shared_home/r08922168/.local/share/jupyter/runtime/kernel-043a19fb-de14-431e-a14f-359f6d7484a6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_home/r08922168/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
      "/shared_home/r08922168/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
      "/shared_home/r08922168/miniconda3/envs/my_paraphrase_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# parse argument\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--config_path\", dest=\"config_path\",\n",
    "                    default='../configs/dpng_transformer_bert_tokenizer_bow_indivtopk.yaml')\n",
    "parser.add_argument(\"--preprocessed\", dest=\"preprocessed\", action=\"store_true\")\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", default=0, type=int)\n",
    "\n",
    "args = parser.parse_args()\n",
    "config_path = args.config_path\n",
    "preprocessed = args.preprocessed\n",
    "seed = args.seed\n",
    "print(\"config_path:\", config_path)\n",
    "print(\"preprocessed: \", preprocessed)\n",
    "print(\"seed: \", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_model_path': '../models/DNPG_base_transformer.pth', 'log_file': '../logs/DNPG_base_transformer_training.txt', 'test_output_file': '../outputs/test_DNPG_base_transformer.txt', 'val_output_file': '../outputs/val_DNPG_base_transformer.txt', 'dataset': 'quora_dataset', 'num_epochs': 50, 'batch_size': 128, 'd_model': 450, 'd_inner_hid': 512, 'd_k': 50, 'd_v': 50, 'n_head': 9, 'n_layers': 3, 'n_warmup_steps': 12000, 'dropout': 0.1, 'embs_share_weight': True, 'proj_share_weight': True, 'label_smoothing': False, 'train_size': 100000, 'val_size': 4000, 'test_size': 20000, 'is_bow': False, 'lr': '1e-3'}\n"
     ]
    }
   ],
   "source": [
    "##### Read Arguments from Config File #####\n",
    "\n",
    "# read from command line\n",
    "\n",
    "# config_path = '../configs/base_transformer.yaml'\n",
    "# config_path = '../configs/dpng_transformer.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indiv_neighbors.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_maskword_indivtopk.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk_onlybow.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk_replace.yaml'\n",
    "# config_path='../configs/dpng_transformer_bert_tokenizer_bow_indivtopk_replace_nopreprocess.yaml'\n",
    "# config_path = '../configs/dpng_transformer_wordnet.yaml'\n",
    "# config_path = '../configs/dpng_transformer_wordnet_replace_nopreprocess.yaml'\n",
    "# config_path = '../configs/dpng_transformer_bert_tokenizer_bow_indivtopk_replace_nopreprocess_no_append_bow.yaml'\n",
    "\n",
    "# preprocessed = False\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config)\n",
    "\n",
    "save_model_path = config['save_model_path']\n",
    "log_file = config['log_file']\n",
    "use_dataset = config['dataset']\n",
    "\n",
    "num_epochs = config['num_epochs']\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "d_model = config['d_model']\n",
    "d_inner_hid = config['d_inner_hid']\n",
    "d_k = config['d_k']\n",
    "d_v = config['d_v']\n",
    "\n",
    "n_head = config['n_head']\n",
    "n_layers = config['n_layers']\n",
    "n_warmup_steps = config['n_warmup_steps']\n",
    "\n",
    "dropout = config['dropout']\n",
    "embs_share_weight = config['embs_share_weight']\n",
    "proj_share_weight = config['proj_share_weight']\n",
    "label_smoothing = config['label_smoothing']\n",
    "\n",
    "train_size = config['train_size']\n",
    "val_size = config['val_size']\n",
    "\n",
    "try:\n",
    "    is_bow = config['is_bow']\n",
    "\n",
    "    if is_bow:\n",
    "        bow_strategy = config['bow_strategy']\n",
    "        topk = config['topk']\n",
    "        if bow_strategy != 'simple_sum':\n",
    "            indiv_topk = config['indiv_topk']\n",
    "        else:\n",
    "            # not used but use default value for simplicity\n",
    "            indiv_topk = 50\n",
    "        \n",
    "        only_bow = config['only_bow']\n",
    "        replace_predict = config['replace_predict']\n",
    "        append_bow = config['append_bow']\n",
    "        \n",
    "except KeyError:\n",
    "    is_bow = False\n",
    "    \n",
    "try:\n",
    "    use_wordnet = config['use_wordnet']\n",
    "    indiv_k = config['indiv_k']\n",
    "    replace_origin = config['replace_origin']\n",
    "    append_bow = config['append_bow']\n",
    "except KeyError:\n",
    "    use_wordnet = False\n",
    "\n",
    "# todo: add to params\n",
    "lr = float(config['lr'])\n",
    "# lr = 5e-4\n",
    "# ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# batch_size = 50\n",
    "# n_warmup_steps = 30000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/DNPG_base_transformer_bert_tokenizer_bert_bow_indivtopk_replace_nopreprocess.pth\n",
      "../logs/DNPG_base_transformer_bert_tokenizer_training_bert_bow_indiv_topk_replace_nopreprocess.txt\n",
      "../models/fixseed/seed0/DNPG_base_transformer_bert_tokenizer_bert_bow_indivtopk_replace_nopreprocess.pth\n",
      "../logs/fixseed/seed0/DNPG_base_transformer_bert_tokenizer_training_bert_bow_indiv_topk_replace_nopreprocess.txt\n"
     ]
    }
   ],
   "source": [
    "# same seed\n",
    "# seed = 0\n",
    "same_seeds(seed)\n",
    "\n",
    "# set model and log path\n",
    "seed_model_root = '../models/fixseed/seed{}/'.format(seed)\n",
    "seed_log_root = '../logs/fixseed/seed{}/'.format(seed)\n",
    "\n",
    "if not os.path.exists(seed_model_root):\n",
    "    os.makedirs(seed_model_root)\n",
    "\n",
    "if not os.path.exists(seed_log_root):\n",
    "    os.makedirs(seed_log_root)\n",
    "\n",
    "save_model_path = seed_model_root + save_model_path.split('/')[-1]\n",
    "log_file = seed_log_root + log_file.split('/')[-1]\n",
    "print('seed: ', seed)\n",
    "print('save model path: ', save_model_path)\n",
    "print('log path: ', log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# preprocessed = False\n",
    "if preprocessed:\n",
    "    from datasets.quora_preprocessed_dataset import QuoraPreprocessedDataset as Dataset\n",
    "else:\n",
    "    if use_dataset == 'quora_dataset':\n",
    "        from datasets.quora_dataset import QuoraDataset as Dataset\n",
    "    elif use_dataset == 'quora_bert_dataset':\n",
    "        from datasets.quora_bert_dataset import QuoraBertDataset as Dataset\n",
    "    elif use_dataset == 'quora_bert_mask_predict_dataset':\n",
    "        from datasets.quora_bert_mask_predict_dataset import QuoraBertMaskPredictDataset as Dataset\n",
    "    elif use_dataset == 'quora_word_mask_prediction_dataset':\n",
    "        from datasets.quora_word_mask_prediction_dataset import QuoraWordMaskPredictDataset as Dataset\n",
    "    elif use_dataset == 'quora_wordnet_dataset':\n",
    "        from datasets.quora_wordnet_dataset import QuoraWordnetDataset as Dataset\n",
    "    elif use_dataset == 'quora_wordnet_aug_dataset':\n",
    "        from datasets.quora_wordnet_aug_dataset import QuoraWordnetAugDataset as Dataset        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset is not defined or not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    seq1_tensors = [s[0] for s in samples]\n",
    "    seq2_tensors = [s[1] for s in samples]\n",
    "\n",
    "    # zero pad\n",
    "    seq1_tensors = pad_sequence(seq1_tensors,\n",
    "                                  batch_first=True)\n",
    "\n",
    "    seq2_tensors = pad_sequence(seq2_tensors,\n",
    "                                  batch_first=True)    \n",
    "    \n",
    "    return seq1_tensors, seq2_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessed:\n",
    "    model_name = config_path.split('/')[-1][:-5]\n",
    "    preprocessed_file = '../data/preprocess_all_{}.npy'.format(model_name)\n",
    "    dataset = Dataset(\"train\", train_size, val_size, preprocessed_file=preprocessed_file)\n",
    "    val_dataset = Dataset(\"val\", train_size, val_size, preprocessed_file=preprocessed_file)    \n",
    "elif is_bow:\n",
    "    dataset = Dataset(\n",
    "        \"train\", train_size, val_size, bow_strategy=bow_strategy, topk=topk, indiv_topk=indiv_topk, \n",
    "        only_bow=only_bow, use_origin=only_bow, replace_predict=replace_predict, append_bow=append_bow\n",
    "    )\n",
    "    # try not to replace predict when validation?\n",
    "    val_dataset = Dataset(\n",
    "        \"val\", train_size, val_size, bow_strategy=bow_strategy, topk=topk, indiv_topk=indiv_topk, \n",
    "        only_bow=only_bow, use_origin=only_bow, replace_predict=False, append_bow=append_bow\n",
    "    )\n",
    "elif use_wordnet:\n",
    "    dataset = Dataset(\"train\", train_size, val_size, indiv_k=indiv_k, replace_origin=replace_origin, append_bow=append_bow)\n",
    "    val_dataset = Dataset(\"val\", train_size, val_size, indiv_k=indiv_k, replace_origin=replace_origin, append_bow=append_bow)\n",
    "else:\n",
    "    dataset = Dataset(\"train\", train_size, val_size)\n",
    "    val_dataset = Dataset(\"val\", train_size, val_size)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    dataset.n_words,\n",
    "    dataset.n_words,\n",
    "    src_pad_idx=dataset.PAD_token_id,\n",
    "    trg_pad_idx=dataset.PAD_token_id,\n",
    "    trg_emb_prj_weight_sharing=proj_share_weight,\n",
    "    emb_src_trg_weight_sharing=embs_share_weight,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    d_model=d_model,\n",
    "    d_word_vec=d_model,\n",
    "    d_inner=d_inner_hid,\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09, lr=lr),\n",
    "    2.0, d_model, n_warmup_steps)\n",
    "# optimizer = optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train epoch\n",
    "def train_epoch(model, data_loader, optimizer, device, smoothing=False):\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    for seq1, seq2 in tqdm(data_loader):\n",
    "        src_seq = seq1.to(device)\n",
    "        trg_seq = seq2[:, :-1].to(device)\n",
    "        gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            pred = model(src_seq, trg_seq)\n",
    "        except RuntimeError as e:\n",
    "#             print(src_seq, trg_seq)\n",
    "            print(\"[Info] Length of src seq: {}, trg seq: {}\".format(len(src_seq), len(trg_seq)))\n",
    "            print(e)\n",
    "            # sentence too long, skip the training batch\n",
    "            continue\n",
    "#             raise RuntimeError(e)\n",
    "            \n",
    "        try:\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, dataset.PAD_token_id, smoothing) \n",
    "            loss.backward()\n",
    "    #         optimizer.step()\n",
    "            optimizer.step_and_update_lr()\n",
    "        # CUDA out of memory\n",
    "        except RuntimeError as e:\n",
    "            print(\"[Info] Length of src seq: {}, trg seq: {}\".format(len(src_seq), len(trg_seq)))\n",
    "            print(e)\n",
    "            # sentence too long, skip the training batch\n",
    "            continue\n",
    "\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    \n",
    "    return loss_per_word, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_data_loader, device):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2 in tqdm(val_data_loader):\n",
    "\n",
    "            src_seq = seq1.to(device)\n",
    "            trg_seq = seq2[:, :-1].to(device)\n",
    "            gold = seq2[:, 1:].contiguous().view(-1).to(device)\n",
    "\n",
    "            pred = model(src_seq, trg_seq)\n",
    "\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, dataset.PAD_token_id, smoothing=False) \n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reproduce\n",
    "# import random\n",
    "# random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(log_file, 'w')\n",
    "# f = open('../logs/tmp.txt', 'w')\n",
    "\n",
    "best_loss = 999\n",
    "\n",
    "f.write(\"Config: {}\\n\".format(config))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {} / {}\".format(epoch + 1, num_epochs))\n",
    "    start = time.time()\n",
    "    train_loss, train_accu = train_epoch(model, data_loader, optimizer, device, smoothing=label_smoothing)\n",
    "    log_performances('Training', train_loss, train_accu, start, f)\n",
    "\n",
    "    start = time.time()\n",
    "    valid_loss, valid_accu = eval_epoch(model, val_data_loader, device)\n",
    "    log_performances('Validation', valid_loss, valid_accu, start, f)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        best_loss = valid_loss\n",
    "        print(\"model saved in Epoch {}\".format(epoch + 1))\n",
    "        f.write(\"model saved in Epoch {}\\n\".format(epoch + 1))\n",
    "        f.flush()\n",
    "\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
