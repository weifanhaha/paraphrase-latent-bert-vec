{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer.SubLayers import MultiHeadAttention, PositionwiseFeedForward\n",
    "from transformer.Models import get_pad_mask, get_subsequent_mask\n",
    "from transformer.Models import PositionalEncoding, Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QKV: dec_out  enc_out  bert_out\n",
    "class DecoderLayer(nn.Module):\n",
    "    ''' Compose with three layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1, bert_alpha=0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.bert_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "        self.bert_alpha = bert_alpha\n",
    "\n",
    "    def forward(\n",
    "            self, dec_input, enc_output, bert_out,\n",
    "            slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        \n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        \n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        \n",
    "        dec_bert_output, dec_enc_bert_attn = self.bert_attn(\n",
    "            dec_output, enc_output, bert_out, mask=dec_enc_attn_mask)\n",
    "        \n",
    "        dec_output = (1-self.bert_alpha) *  dec_output + self.bert_alpha * dec_bert_output\n",
    "        \n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' A decoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, n_position=200, dropout=0.1, bert_alpha=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout, bert_alpha=bert_alpha)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, bert_out, src_mask, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq)))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "        \n",
    "#         print('encoder output', enc_output.shape)\n",
    "#         print('decoder output', dec_output.shape)\n",
    "#         print('bert output', bert_out.shape)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, bert_out, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "            d_word_vec=512, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n",
    "            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True, bert_alpha=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "        \n",
    "        self.probnn = nn.Linear(n_src_vocab, d_model)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=src_pad_idx, dropout=dropout)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=trg_pad_idx, dropout=dropout, bert_alpha=bert_alpha)\n",
    "\n",
    "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        self.x_logit_scale = 1.\n",
    "        if trg_emb_prj_weight_sharing:\n",
    "            # Share the weight between target word embedding & last dense layer\n",
    "            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n",
    "            self.x_logit_scale = (d_model ** -0.5)\n",
    "\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n",
    "\n",
    "\n",
    "    def forward(self, src_seq, trg_seq, bert_prob, return_attns=False):\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_output, enc_slf_attn_list = self.encoder(src_seq, src_mask, return_attns=True)\n",
    "        \n",
    "        bert_out = self.probnn(bert_prob)\n",
    "#         print(\"bert prod:\", bert_prob.shape)\n",
    "#         print(\"(trans) bert out: \", bert_out.shape)\n",
    "        \n",
    "        dec_output, dec_slf_attn_list, dec_enc_attn_list = self.decoder(trg_seq, trg_mask, enc_output, bert_out, src_mask, return_attns=True)\n",
    "        seq_logit = self.trg_word_prj(dec_output) * self.x_logit_scale\n",
    "\n",
    "        if return_attns:\n",
    "            return seq_logit.view(-1, seq_logit.size(2)), enc_slf_attn_list, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return seq_logit.view(-1, seq_logit.size(2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    ''' Load a trained model and translate in beam search fashion. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, model, beam_size, max_seq_len,\n",
    "            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n",
    "        \n",
    "\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.alpha = 0.7\n",
    "        self.beam_size = beam_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_bos_idx = trg_bos_idx\n",
    "        self.trg_eos_idx = trg_eos_idx\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n",
    "        self.register_buffer(\n",
    "            'blank_seqs', \n",
    "            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n",
    "        self.blank_seqs[:, 0] = self.trg_bos_idx\n",
    "        self.register_buffer(\n",
    "            'len_map', \n",
    "            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n",
    "\n",
    "\n",
    "    def _model_decode(self, trg_seq, enc_output, bert_out, src_mask):\n",
    "        trg_mask = get_subsequent_mask(trg_seq)\n",
    "#         dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask, return_attns=True)\n",
    "        bert_out = self.model.probnn(bert_out)\n",
    "        dec_output, dec_slf_attn_list, dec_enc_attn_list = self.model.decoder(trg_seq, trg_mask, enc_output, bert_out, src_mask, return_attns=True)\n",
    "        \n",
    "        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1), dec_enc_attn_list\n",
    "\n",
    "\n",
    "    def _get_init_state(self, src_seq, src_mask, bert_out):\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n",
    "        dec_output, dec_enc_attn_list = self._model_decode(self.init_seq, enc_output, bert_out, src_mask)\n",
    "        \n",
    "        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        scores = torch.log(best_k_probs).view(beam_size)\n",
    "        gen_seq = self.blank_seqs.clone().detach()\n",
    "        gen_seq[:, 1] = best_k_idx[0]\n",
    "        enc_output = enc_output.repeat(beam_size, 1, 1)\n",
    "        bert_out = bert_out.repeat(beam_size, 1, 1)\n",
    "        return enc_output, gen_seq, scores, dec_enc_attn_list, bert_out\n",
    "\n",
    "\n",
    "    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n",
    "        assert len(scores.size()) == 1\n",
    "        \n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        # Get k candidates for each beam, k^2 candidates in total.\n",
    "        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        # Include the previous scores.\n",
    "        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n",
    "\n",
    "        # Get the best k candidates from k^2 candidates.\n",
    "        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n",
    " \n",
    "        # Get the corresponding positions of the best k candidiates.\n",
    "        best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n",
    "        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n",
    "\n",
    "        # Copy the corresponding previous tokens.\n",
    "        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n",
    "        # Set the best tokens in this beam search step\n",
    "        gen_seq[:, step] = best_k_idx\n",
    "\n",
    "        return gen_seq, scores\n",
    "\n",
    "\n",
    "    def translate_sentence(self, src_seq, bert_out, return_attn=False):\n",
    "        # Only accept batch size equals to 1 in this function.\n",
    "        # TODO: expand to batch operation.\n",
    "        assert src_seq.size(0) == 1\n",
    "\n",
    "        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n",
    "        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n",
    "        \n",
    "        with torch.no_grad():\n",
    "        #     pred, enc_slf_attn_list, dec_slf_attn_list, dec_enc_attn_list = model(src_seq, trg_seq, True)\n",
    "        #     pred_seq = translator.translate_sentence(src_seq)\n",
    "            src_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "            enc_output, gen_seq, scores, dec_enc_attn_list, bert_out = self._get_init_state(src_seq, src_mask, bert_out)\n",
    "\n",
    "            output_dec_enc_attn = [[] for _ in range(len(dec_enc_attn_list))]\n",
    "            for l in range(len(dec_enc_attn_list)):\n",
    "                    output_dec_enc_attn[l].append(dec_enc_attn_list[l][0,:,-1,:])    \n",
    "            ans_idx = 0   # default\n",
    "\n",
    "\n",
    "            for step in range(2, max_seq_len):    # decode up to max length\n",
    "#                 print('enc_out shape: ', enc_output.shape)\n",
    "#                 print('bert_out shape: ', bert_out.shape)\n",
    "\n",
    "                dec_output, dec_enc_attn_list = self._model_decode(gen_seq[:, :step], enc_output, bert_out, src_mask)\n",
    "\n",
    "                gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n",
    "                \n",
    "                # append dec_enc_attn_list\n",
    "                for l in range(len(dec_enc_attn_list)):\n",
    "                    output_dec_enc_attn[l].append(dec_enc_attn_list[l][0,:,-1,:])\n",
    "\n",
    "                # Check if all path finished\n",
    "                # -- locate the eos in the generated sequences\n",
    "                eos_locs = gen_seq == trg_eos_idx   \n",
    "                # -- replace the eos with its position for the length penalty use\n",
    "                seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n",
    "                # -- check if all beams contain eos\n",
    "                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n",
    "                    # TODO: Try different terminate conditions.\n",
    "                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n",
    "                    ans_idx = ans_idx.item()\n",
    "                    break\n",
    "\n",
    "\n",
    "        if return_attn:\n",
    "            for l in range(len(output_dec_enc_attn)):\n",
    "                output_dec_enc_attn[l] = torch.stack(output_dec_enc_attn[l], dim=1)\n",
    "            return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist(), output_dec_enc_attn\n",
    "        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
